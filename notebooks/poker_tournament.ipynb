{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poker LLM Tournament\n",
    "\n",
    "Single-elimination tournament comparing poker LLMs with **1000 hands per matchup**.\n",
    "\n",
    "## Models\n",
    "| Model | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| Qwen3-Base | HuggingFace | `unsloth/Qwen3-4B-Thinking-2507` - Base thinking model |\n",
    "| Qwen3-SFT | HuggingFace | `YiPz/qwen3-4b-pokergpt-o3-sft-lora` - Fine-tuned on 5k hands |\n",
    "| Llama3-SFT | HuggingFace | `YiPz/llama3-8b-pokerbench-sft` - From PokerBench paper |\n",
    "| GPT-4 | OpenAI API | Only runs if your model beats Llama3 |\n",
    "\n",
    "## Gauntlet Format (Cost Optimized)\n",
    "```\n",
    "Round 1: Qwen3-Base vs Qwen3-SFT     (your models head-to-head)\n",
    "Round 2: Winner R1 vs Llama3-SFT     (benchmark test)\n",
    "Round 3: Winner R2 vs GPT-4          (only if your model wins R2)\n",
    "```\n",
    "\n",
    "**Winner determined by total chip profit after 1000 hands.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for model caching\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers accelerate torch pokerkit\n",
    "!pip install -q tqdm pandas matplotlib openai\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/content/drive/MyDrive/hf_cache\"\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key from Colab secrets\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    print(\"OpenAI API key loaded from secrets\")\n",
    "except:\n",
    "    print(\"Warning: OPENAI_API_KEY not found in secrets. GPT-4 matchup will be skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "\n",
    "def detect_gpu():\n",
    "    \"\"\"Detect GPU and VRAM.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        gpu_name, vram_mb = result.stdout.strip().split(\", \")\n",
    "        vram_gb = float(vram_mb) / 1024\n",
    "    except:\n",
    "        gpu_name, vram_gb = \"Unknown\", 16.0\n",
    "    return gpu_name, vram_gb\n",
    "\n",
    "GPU_NAME, VRAM_GB = detect_gpu()\n",
    "print(f\"GPU: {GPU_NAME} ({VRAM_GB:.0f}GB)\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check if we have enough VRAM for full weight Llama3-8B (~16GB)\n",
    "if VRAM_GB < 20:\n",
    "    print(f\"\\nWarning: Llama3-8B requires ~16GB VRAM at FP16.\")\n",
    "    print(\"Models will be loaded/unloaded sequentially to manage memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tournament Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tournament settings\n",
    "HANDS_PER_MATCHUP = 1000\n",
    "STARTING_STACK = 10000\n",
    "SMALL_BLIND = 50\n",
    "BIG_BLIND = 100\n",
    "SEED = 42\n",
    "VERBOSE = False  # Set True to see each action\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"Qwen3-Base\": {\n",
    "        \"type\": \"transformers\",\n",
    "        \"model_id\": \"unsloth/Qwen3-4B-Thinking-2507\",\n",
    "    },\n",
    "    \"Qwen3-SFT\": {\n",
    "        \"type\": \"transformers\",\n",
    "        \"model_id\": \"YiPz/qwen3-4b-pokergpt-o3-sft-lora\",\n",
    "    },\n",
    "    \"Llama3-SFT\": {\n",
    "        \"type\": \"transformers\",\n",
    "        \"model_id\": \"YiPz/llama3-8b-pokerbench-sft\",\n",
    "    },\n",
    "    \"GPT-4\": {\n",
    "        \"type\": \"openai\",\n",
    "        \"model\": \"gpt-4\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Gauntlet order\n",
    "GAUNTLET = [\n",
    "    (\"Qwen3-Base\", \"Qwen3-SFT\"),   # Round 1: Your models\n",
    "    (\"WINNER_R1\", \"Llama3-SFT\"),    # Round 2: vs Benchmark\n",
    "    (\"WINNER_R2\", \"GPT-4\"),         # Round 3: vs GPT-4 (conditional)\n",
    "]\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"/content/tournament_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/observability/traces\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/charts\", exist_ok=True)\n",
    "\n",
    "print(f\"Tournament Config:\")\n",
    "print(f\"  Hands per matchup: {HANDS_PER_MATCHUP}\")\n",
    "print(f\"  Stack: {STARTING_STACK}\")\n",
    "print(f\"  Blinds: {SMALL_BLIND}/{BIG_BLIND}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import gc\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from pokerkit import NoLimitTexasHoldem, Automation\n",
    "\n",
    "\n",
    "# ============= Action Parsing =============\n",
    "\n",
    "@dataclass\n",
    "class ParsedAction:\n",
    "    action_type: str\n",
    "    amount: Optional[int] = None\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.amount:\n",
    "            return f\"{self.action_type.title()} {self.amount}\"\n",
    "        return self.action_type.title()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ParseResult:\n",
    "    action: ParsedAction\n",
    "    method: str  # \"tag\" | \"regex_*\" | \"default\"\n",
    "    raw_match: str\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "class ActionParser:\n",
    "    RE_ACTION_TAG = re.compile(r\"<action>\\s*([^<]+?)\\s*</action>\", re.IGNORECASE)\n",
    "    RE_FOLD = re.compile(r\"\\b(f|fold)\\b\", re.IGNORECASE)\n",
    "    RE_CC = re.compile(r\"\\b(cc|call|check)\\b\", re.IGNORECASE)\n",
    "    RE_CBR = re.compile(r\"\\b(?:cbr|bet|raise)(?:\\s+(?:to\\s+)?(\\d+))?\\b\", re.IGNORECASE)\n",
    "    RE_ALL_IN = re.compile(r\"\\b(all[\\-\\s]?in|shove)\\b\", re.IGNORECASE)\n",
    "\n",
    "    def parse(self, text: str, can_check: bool = True, stack: int = 0) -> ParsedAction:\n",
    "        return self.parse_with_metadata(text, can_check, stack).action\n",
    "\n",
    "    def parse_with_metadata(self, text: str, can_check: bool = True, stack: int = 0) -> ParseResult:\n",
    "        tag_match = self.RE_ACTION_TAG.search(text)\n",
    "        used_tag = tag_match is not None\n",
    "        content = tag_match.group(1).strip() if tag_match else text\n",
    "\n",
    "        if self.RE_ALL_IN.search(content):\n",
    "            return ParseResult(ParsedAction(\"all_in\", stack), \"tag\" if used_tag else \"regex_allin\", content)\n",
    "        if self.RE_FOLD.search(content):\n",
    "            return ParseResult(ParsedAction(\"fold\"), \"tag\" if used_tag else \"regex_fold\", content)\n",
    "        if self.RE_CC.search(content):\n",
    "            action = ParsedAction(\"check\" if can_check else \"call\")\n",
    "            return ParseResult(action, \"tag\" if used_tag else \"regex_call\", content)\n",
    "\n",
    "        cbr = self.RE_CBR.search(content)\n",
    "        if cbr:\n",
    "            amt = int(cbr.group(1)) if cbr.group(1) else stack\n",
    "            return ParseResult(ParsedAction(\"raise\", amt), \"tag\" if used_tag else \"regex_raise\", content)\n",
    "\n",
    "        default_action = ParsedAction(\"check\" if can_check else \"fold\")\n",
    "        return ParseResult(default_action, \"default\", content[:100], \"No valid action pattern found\")\n",
    "\n",
    "\n",
    "# ============= Action Record =============\n",
    "\n",
    "@dataclass\n",
    "class ActionRecord:\n",
    "    hand_id: int\n",
    "    street: str\n",
    "    hole_cards: Tuple[str, str]\n",
    "    board: List[str]\n",
    "    pot: int\n",
    "    to_call: int\n",
    "    stack: int\n",
    "    position: str\n",
    "    action: ParsedAction\n",
    "    thinking: str\n",
    "    response: str\n",
    "    latency_ms: float\n",
    "    tokens_generated: int\n",
    "    parse_method: str = \"unknown\"\n",
    "    parse_error: Optional[str] = None\n",
    "\n",
    "\n",
    "print(\"Core classes loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Observability =============\n",
    "\n",
    "@dataclass\n",
    "class ModelObservability:\n",
    "    model_name: str\n",
    "    total_actions: int = 0\n",
    "    valid_tag_parses: int = 0\n",
    "    regex_fallback_parses: int = 0\n",
    "    default_fallback_parses: int = 0\n",
    "    action_execution_failures: int = 0\n",
    "    empty_responses: int = 0\n",
    "    fold_count: int = 0\n",
    "    check_count: int = 0\n",
    "    call_count: int = 0\n",
    "    raise_count: int = 0\n",
    "    all_in_count: int = 0\n",
    "    latencies: List[float] = field(default_factory=list)\n",
    "    total_tokens: int = 0\n",
    "\n",
    "    @property\n",
    "    def parse_error_rate(self) -> float:\n",
    "        if self.total_actions == 0:\n",
    "            return 0.0\n",
    "        return (self.regex_fallback_parses + self.default_fallback_parses) / self.total_actions\n",
    "\n",
    "    @property\n",
    "    def avg_latency_ms(self) -> float:\n",
    "        return sum(self.latencies) / len(self.latencies) if self.latencies else 0.0\n",
    "\n",
    "    @property\n",
    "    def p99_latency_ms(self) -> float:\n",
    "        if not self.latencies:\n",
    "            return 0.0\n",
    "        sorted_lat = sorted(self.latencies)\n",
    "        return sorted_lat[int(len(sorted_lat) * 0.99)]\n",
    "\n",
    "\n",
    "class ObservabilityCollector:\n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.traces: Dict[str, List[dict]] = {}\n",
    "        self.metrics: Dict[str, ModelObservability] = {}\n",
    "\n",
    "    def record_action(self, model_name: str, record: ActionRecord, executed_action: str, fallback_used: bool):\n",
    "        # Store trace\n",
    "        if model_name not in self.traces:\n",
    "            self.traces[model_name] = []\n",
    "\n",
    "        trace = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"hand_id\": record.hand_id,\n",
    "            \"street\": record.street,\n",
    "            \"hole_cards\": list(record.hole_cards),\n",
    "            \"board\": record.board,\n",
    "            \"pot\": record.pot,\n",
    "            \"to_call\": record.to_call,\n",
    "            \"stack\": record.stack,\n",
    "            \"position\": record.position,\n",
    "            \"raw_response\": record.response,\n",
    "            \"thinking\": record.thinking,\n",
    "            \"parsed_action\": record.action.action_type,\n",
    "            \"parsed_amount\": record.action.amount,\n",
    "            \"parse_method\": record.parse_method,\n",
    "            \"parse_error\": record.parse_error,\n",
    "            \"executed_action\": executed_action,\n",
    "            \"fallback_used\": fallback_used,\n",
    "            \"latency_ms\": record.latency_ms,\n",
    "            \"tokens\": record.tokens_generated,\n",
    "        }\n",
    "        self.traces[model_name].append(trace)\n",
    "\n",
    "        # Update metrics\n",
    "        if model_name not in self.metrics:\n",
    "            self.metrics[model_name] = ModelObservability(model_name=model_name)\n",
    "        m = self.metrics[model_name]\n",
    "\n",
    "        m.total_actions += 1\n",
    "        m.latencies.append(record.latency_ms)\n",
    "        m.total_tokens += record.tokens_generated\n",
    "\n",
    "        if record.parse_method == \"tag\":\n",
    "            m.valid_tag_parses += 1\n",
    "        elif record.parse_method.startswith(\"regex\"):\n",
    "            m.regex_fallback_parses += 1\n",
    "        elif record.parse_method == \"default\":\n",
    "            m.default_fallback_parses += 1\n",
    "\n",
    "        if not record.response.strip():\n",
    "            m.empty_responses += 1\n",
    "        if fallback_used:\n",
    "            m.action_execution_failures += 1\n",
    "\n",
    "        action = executed_action.lower()\n",
    "        if action == \"fold\": m.fold_count += 1\n",
    "        elif action == \"check\": m.check_count += 1\n",
    "        elif action == \"call\": m.call_count += 1\n",
    "        elif action == \"raise\": m.raise_count += 1\n",
    "        elif action == \"all_in\": m.all_in_count += 1\n",
    "\n",
    "    def write_traces(self, matchup_id: str):\n",
    "        traces_dir = self.output_dir / \"observability\" / \"traces\"\n",
    "        traces_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for model_name, traces in self.traces.items():\n",
    "            safe_name = model_name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "            filepath = traces_dir / f\"{safe_name}_{matchup_id}.jsonl\"\n",
    "            with open(filepath, \"w\") as f:\n",
    "                for trace in traces:\n",
    "                    f.write(json.dumps(trace) + \"\\n\")\n",
    "\n",
    "    def export_metrics(self):\n",
    "        metrics_path = self.output_dir / \"observability\" / \"model_metrics.json\"\n",
    "        data = {}\n",
    "        for name, m in self.metrics.items():\n",
    "            data[name] = {\n",
    "                \"total_actions\": m.total_actions,\n",
    "                \"valid_tag_parses\": m.valid_tag_parses,\n",
    "                \"regex_fallback_parses\": m.regex_fallback_parses,\n",
    "                \"default_fallback_parses\": m.default_fallback_parses,\n",
    "                \"parse_error_rate\": round(m.parse_error_rate, 4),\n",
    "                \"empty_responses\": m.empty_responses,\n",
    "                \"action_execution_failures\": m.action_execution_failures,\n",
    "                \"action_distribution\": {\n",
    "                    \"fold\": m.fold_count, \"check\": m.check_count,\n",
    "                    \"call\": m.call_count, \"raise\": m.raise_count, \"all_in\": m.all_in_count,\n",
    "                },\n",
    "                \"avg_latency_ms\": round(m.avg_latency_ms, 2),\n",
    "                \"p99_latency_ms\": round(m.p99_latency_ms, 2),\n",
    "                \"total_tokens\": m.total_tokens,\n",
    "            }\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def clear(self):\n",
    "        self.traces = {}\n",
    "        self.metrics = {}\n",
    "\n",
    "\n",
    "print(\"Observability loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= TransformersPlayer =============\n",
    "\n",
    "class TransformersPlayer:\n",
    "    SYSTEM_PROMPT = \"\"\"You are an expert poker player. Analyze the game state and decide your action.\n",
    "\n",
    "Output format: <action>ACTION</action>\n",
    "- <action>f</action> = fold\n",
    "- <action>cc</action> = check or call\n",
    "- <action>cbr X</action> = bet or raise to X (multiple of big blind)\n",
    "\n",
    "VALID:\n",
    "<action>f</action>\n",
    "<action>cc</action>\n",
    "<action>cbr 6</action>\n",
    "\n",
    "INVALID:\n",
    "<action>fold</action> -- NOT PHH FORMAT\n",
    "<action>p6 cc</action> -- DO NOT SPECIFY PLAYER\n",
    "<action>cbr 1 5</action> -- INVALID PHH\n",
    "\n",
    "Think step by step, then output exactly ONE action tag.\"\"\"\n",
    "\n",
    "    THINK_END_TOKEN_ID = 151668\n",
    "\n",
    "    def __init__(self, name: str, model: Any, tokenizer: Any, temperature: float = 0.6, max_new_tokens: int = 512):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.parser = ActionParser()\n",
    "        self.action_history: List[ActionRecord] = []\n",
    "        self._hand_id = 0\n",
    "        self._street = \"preflop\"\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def set_hand_context(self, hand_id: int, street: str):\n",
    "        self._hand_id = hand_id\n",
    "        self._street = street\n",
    "\n",
    "    def get_action(self, hole_cards, board, pot, to_call, stack, position, num_players) -> ParsedAction:\n",
    "        start = time.perf_counter()\n",
    "        prompt = self._build_prompt(hole_cards, board, pot, to_call, stack, position, num_players)\n",
    "\n",
    "        try:\n",
    "            thinking, response, tokens_gen = self._generate(prompt)\n",
    "            can_check = to_call == 0\n",
    "            result = self.parser.parse_with_metadata(response, can_check, stack)\n",
    "            action = result.action\n",
    "            parse_method = result.method\n",
    "            parse_error = result.error\n",
    "        except Exception as e:\n",
    "            thinking, response, tokens_gen = \"\", f\"ERROR: {e}\", 0\n",
    "            action = ParsedAction(\"fold\")\n",
    "            parse_method = \"error\"\n",
    "            parse_error = str(e)\n",
    "\n",
    "        latency = (time.perf_counter() - start) * 1000\n",
    "\n",
    "        self.action_history.append(ActionRecord(\n",
    "            hand_id=self._hand_id, street=self._street, hole_cards=hole_cards,\n",
    "            board=list(board), pot=pot, to_call=to_call, stack=stack,\n",
    "            position=position, action=action, thinking=thinking[:1000],\n",
    "            response=response[:500], latency_ms=latency, tokens_generated=tokens_gen,\n",
    "            parse_method=parse_method, parse_error=parse_error,\n",
    "        ))\n",
    "        return action\n",
    "\n",
    "    def _build_prompt(self, hole_cards, board, pot, to_call, stack, position, num_players) -> str:\n",
    "        board_str = \" \".join(board) if board else \"None\"\n",
    "        user_msg = f\"\"\"Game: {num_players}-handed No-Limit Hold'em\n",
    "Position: {position}\n",
    "Stack: {stack}\n",
    "Hole Cards: {hole_cards[0]} {hole_cards[1]}\n",
    "Board: {board_str}\n",
    "Pot: {pot}\n",
    "To Call: {to_call}\n",
    "\n",
    "What is your action?\"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_msg}]\n",
    "        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    def _generate(self, prompt: str) -> Tuple[str, str, int]:\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, max_new_tokens=self.max_new_tokens, temperature=self.temperature,\n",
    "                top_p=0.95, top_k=20, do_sample=True, pad_token_id=self.tokenizer.pad_token_id,\n",
    "            )\n",
    "\n",
    "        new_tokens = outputs[0][input_len:]\n",
    "        num_tokens = len(new_tokens)\n",
    "\n",
    "        try:\n",
    "            think_end_idx = (new_tokens == self.THINK_END_TOKEN_ID).nonzero(as_tuple=True)[0][-1].item()\n",
    "            thinking_tokens = new_tokens[:think_end_idx]\n",
    "            response_tokens = new_tokens[think_end_idx + 1:]\n",
    "        except:\n",
    "            thinking_tokens = torch.tensor([], dtype=new_tokens.dtype)\n",
    "            response_tokens = new_tokens\n",
    "\n",
    "        thinking = self.tokenizer.decode(thinking_tokens, skip_special_tokens=True).strip()\n",
    "        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n",
    "        return thinking, response, num_tokens\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        if not self.action_history:\n",
    "            return {}\n",
    "        total = len(self.action_history)\n",
    "        preflop = [a for a in self.action_history if a.street == \"preflop\"]\n",
    "        vpip = len([a for a in preflop if a.action.action_type in (\"call\", \"raise\", \"all_in\")]) / len(preflop) if preflop else 0\n",
    "        pfr = len([a for a in preflop if a.action.action_type in (\"raise\", \"all_in\")]) / len(preflop) if preflop else 0\n",
    "        return {\n",
    "            \"total_actions\": total, \"vpip\": vpip, \"pfr\": pfr,\n",
    "            \"avg_latency_ms\": sum(a.latency_ms for a in self.action_history) / total,\n",
    "            \"fold_pct\": sum(1 for a in self.action_history if a.action.action_type == \"fold\") / total,\n",
    "        }\n",
    "\n",
    "    def get_last_record(self) -> Optional[ActionRecord]:\n",
    "        return self.action_history[-1] if self.action_history else None\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.action_history = []\n",
    "\n",
    "\n",
    "print(\"TransformersPlayer loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= OpenAIPlayer =============\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "\n",
    "\n",
    "class OpenAIPlayer:\n",
    "    SYSTEM_PROMPT = TransformersPlayer.SYSTEM_PROMPT  # Same prompt\n",
    "\n",
    "    def __init__(self, name: str, model: str = \"gpt-4\", temperature: float = 0.6, max_tokens: int = 512):\n",
    "        if not OPENAI_AVAILABLE:\n",
    "            raise ImportError(\"openai package not installed\")\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.client = OpenAI()\n",
    "        self.parser = ActionParser()\n",
    "        self.action_history: List[ActionRecord] = []\n",
    "        self._hand_id = 0\n",
    "        self._street = \"preflop\"\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "\n",
    "    def set_hand_context(self, hand_id: int, street: str):\n",
    "        self._hand_id = hand_id\n",
    "        self._street = street\n",
    "\n",
    "    def get_action(self, hole_cards, board, pot, to_call, stack, position, num_players) -> ParsedAction:\n",
    "        start = time.perf_counter()\n",
    "        user_msg = self._build_prompt(hole_cards, board, pot, to_call, stack, position, num_players)\n",
    "\n",
    "        try:\n",
    "            response_text, tokens_in, tokens_out = self._call_api(user_msg)\n",
    "            can_check = to_call == 0\n",
    "            result = self.parser.parse_with_metadata(response_text, can_check, stack)\n",
    "            action = result.action\n",
    "            parse_method = result.method\n",
    "            parse_error = result.error\n",
    "        except Exception as e:\n",
    "            response_text = f\"ERROR: {e}\"\n",
    "            tokens_in = tokens_out = 0\n",
    "            action = ParsedAction(\"fold\")\n",
    "            parse_method = \"error\"\n",
    "            parse_error = str(e)\n",
    "\n",
    "        latency = (time.perf_counter() - start) * 1000\n",
    "\n",
    "        self.action_history.append(ActionRecord(\n",
    "            hand_id=self._hand_id, street=self._street, hole_cards=hole_cards,\n",
    "            board=list(board), pot=pot, to_call=to_call, stack=stack,\n",
    "            position=position, action=action, thinking=\"\",\n",
    "            response=response_text[:500], latency_ms=latency, tokens_generated=tokens_out,\n",
    "            parse_method=parse_method, parse_error=parse_error,\n",
    "        ))\n",
    "        return action\n",
    "\n",
    "    def _build_prompt(self, hole_cards, board, pot, to_call, stack, position, num_players) -> str:\n",
    "        board_str = \" \".join(board) if board else \"None\"\n",
    "        return f\"\"\"Game: {num_players}-handed No-Limit Hold'em\n",
    "Position: {position}\n",
    "Stack: {stack}\n",
    "Hole Cards: {hole_cards[0]} {hole_cards[1]}\n",
    "Board: {board_str}\n",
    "Pot: {pot}\n",
    "To Call: {to_call}\n",
    "\n",
    "What is your action?\"\"\"\n",
    "\n",
    "    def _call_api(self, user_msg: str) -> Tuple[str, int, int]:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "            ],\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "        )\n",
    "        content = response.choices[0].message.content or \"\"\n",
    "        tokens_in = response.usage.prompt_tokens if response.usage else 0\n",
    "        tokens_out = response.usage.completion_tokens if response.usage else 0\n",
    "        self.total_input_tokens += tokens_in\n",
    "        self.total_output_tokens += tokens_out\n",
    "        return content, tokens_in, tokens_out\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        if not self.action_history:\n",
    "            return {}\n",
    "        total = len(self.action_history)\n",
    "        preflop = [a for a in self.action_history if a.street == \"preflop\"]\n",
    "        vpip = len([a for a in preflop if a.action.action_type in (\"call\", \"raise\", \"all_in\")]) / len(preflop) if preflop else 0\n",
    "        pfr = len([a for a in preflop if a.action.action_type in (\"raise\", \"all_in\")]) / len(preflop) if preflop else 0\n",
    "        return {\n",
    "            \"total_actions\": total, \"vpip\": vpip, \"pfr\": pfr,\n",
    "            \"avg_latency_ms\": sum(a.latency_ms for a in self.action_history) / total,\n",
    "            \"fold_pct\": sum(1 for a in self.action_history if a.action.action_type == \"fold\") / total,\n",
    "            \"total_input_tokens\": self.total_input_tokens,\n",
    "            \"total_output_tokens\": self.total_output_tokens,\n",
    "        }\n",
    "\n",
    "    def get_last_record(self) -> Optional[ActionRecord]:\n",
    "        return self.action_history[-1] if self.action_history else None\n",
    "\n",
    "    def get_estimated_cost(self) -> float:\n",
    "        if \"turbo\" in self.model.lower() or \"4o\" in self.model.lower():\n",
    "            return self.total_input_tokens * 10 / 1e6 + self.total_output_tokens * 30 / 1e6\n",
    "        return self.total_input_tokens * 30 / 1e6 + self.total_output_tokens * 60 / 1e6\n",
    "\n",
    "    def reset_history(self):\n",
    "        self.action_history = []\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "\n",
    "\n",
    "print(f\"OpenAIPlayer loaded! (available: {OPENAI_AVAILABLE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Hand Result & Metrics =============\n",
    "\n",
    "@dataclass\n",
    "class HandResult:\n",
    "    hand_id: int\n",
    "    player_names: List[str]\n",
    "    starting_stacks: List[int]\n",
    "    ending_stacks: List[int]\n",
    "    chip_deltas: List[int]\n",
    "    hole_cards: Dict[str, Tuple[str, str]]\n",
    "    board: List[str]\n",
    "    winner_names: List[str]\n",
    "    pot_size: int\n",
    "\n",
    "\n",
    "class MetricsCollector:\n",
    "    def __init__(self, session_id: str = None):\n",
    "        self.session_id = session_id or f\"session_{int(time.time())}\"\n",
    "        self.hand_results: List[HandResult] = []\n",
    "        self.session_start = time.time()\n",
    "        self.player_summaries = {}\n",
    "\n",
    "    def log_hand(self, result: HandResult):\n",
    "        self.hand_results.append(result)\n",
    "\n",
    "    def finalize_session(self, player_stats: Dict[str, dict]):\n",
    "        duration = time.time() - self.session_start\n",
    "        total_hands = len(self.hand_results)\n",
    "\n",
    "        player_names = set()\n",
    "        for hr in self.hand_results:\n",
    "            player_names.update(hr.player_names)\n",
    "\n",
    "        for name in player_names:\n",
    "            hands_played = hands_won = total_chip_delta = 0\n",
    "            for hr in self.hand_results:\n",
    "                if name in hr.player_names:\n",
    "                    idx = hr.player_names.index(name)\n",
    "                    hands_played += 1\n",
    "                    total_chip_delta += hr.chip_deltas[idx]\n",
    "                    if name in hr.winner_names:\n",
    "                        hands_won += 1\n",
    "\n",
    "            self.player_summaries[name] = {\n",
    "                \"hands_played\": hands_played, \"hands_won\": hands_won,\n",
    "                \"win_rate\": hands_won / hands_played if hands_played > 0 else 0,\n",
    "                \"total_chip_delta\": total_chip_delta,\n",
    "                \"bb_per_100\": (total_chip_delta / hands_played * 100 / BIG_BLIND) if hands_played > 0 else 0,\n",
    "                **player_stats.get(name, {}),\n",
    "            }\n",
    "\n",
    "        self.duration = duration\n",
    "        self.total_hands = total_hands\n",
    "\n",
    "\n",
    "print(\"Metrics loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Eval Game =============\n",
    "\n",
    "class EvalPokerGame:\n",
    "    def __init__(self, players, starting_stack=10000, small_blind=50, big_blind=100,\n",
    "                 metrics=None, observability=None, verbose=False, progress_callback=None):\n",
    "        self.players = players\n",
    "        self.num_players = len(players)\n",
    "        self.starting_stack = starting_stack\n",
    "        self.small_blind = small_blind\n",
    "        self.big_blind = big_blind\n",
    "        self.stacks = [starting_stack] * self.num_players\n",
    "        self.button = 0\n",
    "        self.hand_num = 0\n",
    "        self.metrics = metrics or MetricsCollector()\n",
    "        self.observability = observability\n",
    "        self.verbose = verbose\n",
    "        self.progress_callback = progress_callback\n",
    "\n",
    "    def play_session(self, num_hands: int) -> MetricsCollector:\n",
    "        for hand_idx in range(num_hands):\n",
    "            self._play_hand()\n",
    "            if self.progress_callback:\n",
    "                self.progress_callback(hand_idx + 1, num_hands)\n",
    "            if sum(1 for s in self.stacks if s > 0) < 2:\n",
    "                break\n",
    "        self.metrics.finalize_session({p.name: p.get_stats() for p in self.players})\n",
    "        return self.metrics\n",
    "\n",
    "    def _play_hand(self):\n",
    "        self.hand_num += 1\n",
    "        self.button = (self.button + 1) % self.num_players\n",
    "        for p in self.players:\n",
    "            p.set_hand_context(self.hand_num, \"preflop\")\n",
    "\n",
    "        sb_pos = (self.button + 1) % self.num_players\n",
    "        bb_pos = (self.button + 2) % self.num_players\n",
    "        if self.stacks[sb_pos] <= 0 or self.stacks[bb_pos] <= 0:\n",
    "            return\n",
    "\n",
    "        starting_stacks = self.stacks.copy()\n",
    "\n",
    "        try:\n",
    "            state = NoLimitTexasHoldem.create_state(\n",
    "                automations=(Automation.ANTE_POSTING, Automation.BET_COLLECTION, Automation.BLIND_OR_STRADDLE_POSTING,\n",
    "                             Automation.CARD_BURNING, Automation.HOLE_DEALING, Automation.HOLE_CARDS_SHOWING_OR_MUCKING,\n",
    "                             Automation.HAND_KILLING, Automation.CHIPS_PUSHING, Automation.CHIPS_PULLING),\n",
    "                ante_trimming_status=True, raw_antes={-1: 0},\n",
    "                raw_blinds_or_straddles=(self.small_blind, self.big_blind),\n",
    "                min_bet=self.big_blind, raw_starting_stacks=self.stacks.copy(), player_count=self.num_players,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"Error: {e}\")\n",
    "            return\n",
    "\n",
    "        hole_cards = [(str(state.hole_cards[i][0]), str(state.hole_cards[i][1]))\n",
    "                      if state.hole_cards[i] and len(state.hole_cards[i]) >= 2 else (\"??\", \"??\")\n",
    "                      for i in range(self.num_players)]\n",
    "        dealable = list(state.get_dealable_cards())\n",
    "        random.shuffle(dealable)\n",
    "        deck = dealable\n",
    "        board = []\n",
    "\n",
    "        for street in [\"preflop\", \"flop\", \"turn\", \"river\"]:\n",
    "            if state.status is False:\n",
    "                break\n",
    "            for p in self.players:\n",
    "                p.set_hand_context(self.hand_num, street)\n",
    "\n",
    "            if street == \"flop\":\n",
    "                board = [deck.pop(), deck.pop(), deck.pop()]\n",
    "                for c in board:\n",
    "                    try: state.deal_board(c)\n",
    "                    except: pass\n",
    "            elif street in (\"turn\", \"river\"):\n",
    "                board.append(deck.pop())\n",
    "                try: state.deal_board(board[-1])\n",
    "                except: pass\n",
    "\n",
    "            board_strs = [str(c) for c in board]\n",
    "            while state.actor_index is not None:\n",
    "                actor = state.actor_index\n",
    "                player = self.players[actor]\n",
    "                pot = state.total_pot_amount if hasattr(state, 'total_pot_amount') else 0\n",
    "                current_bet = max(state.bets) if state.bets else 0\n",
    "                player_bet = state.bets[actor] if state.bets else 0\n",
    "                to_call = current_bet - player_bet\n",
    "                stack = state.stacks[actor]\n",
    "                position = self._get_position_name(actor)\n",
    "\n",
    "                action = player.get_action(hole_cards[actor], board_strs, pot, to_call, stack, position, self.num_players)\n",
    "                if self.verbose:\n",
    "                    print(f\"  H{self.hand_num} {street} {player.name}: {action}\")\n",
    "\n",
    "                executed, fallback = self._execute_action(state, action)\n",
    "\n",
    "                # Record observability\n",
    "                if self.observability:\n",
    "                    record = player.get_last_record()\n",
    "                    if record:\n",
    "                        self.observability.record_action(player.name, record, executed, fallback)\n",
    "\n",
    "        if hasattr(state, 'stacks'):\n",
    "            for i in range(self.num_players):\n",
    "                self.stacks[i] = state.stacks[i]\n",
    "\n",
    "        chip_deltas = [self.stacks[i] - starting_stacks[i] for i in range(self.num_players)]\n",
    "        winners = [self.players[i].name for i, d in enumerate(chip_deltas) if d > 0]\n",
    "\n",
    "        self.metrics.log_hand(HandResult(\n",
    "            hand_id=self.hand_num, player_names=[p.name for p in self.players],\n",
    "            starting_stacks=starting_stacks, ending_stacks=self.stacks.copy(),\n",
    "            chip_deltas=chip_deltas, hole_cards={p.name: hole_cards[i] for i, p in enumerate(self.players)},\n",
    "            board=[str(c) for c in board], winner_names=winners, pot_size=sum(abs(d) for d in chip_deltas if d < 0),\n",
    "        ))\n",
    "\n",
    "    def _execute_action(self, state, action: ParsedAction) -> Tuple[str, bool]:\n",
    "        \"\"\"Execute action, return (executed_action_name, used_fallback)\"\"\"\n",
    "        try:\n",
    "            if action.action_type == \"fold\":\n",
    "                state.fold()\n",
    "                return \"fold\", False\n",
    "            elif action.action_type in (\"check\", \"call\"):\n",
    "                state.check_or_call()\n",
    "                return action.action_type, False\n",
    "            elif action.action_type in (\"raise\", \"bet\"):\n",
    "                state.complete_bet_or_raise_to(action.amount)\n",
    "                return \"raise\", False\n",
    "            elif action.action_type == \"all_in\":\n",
    "                actor = state.actor_index\n",
    "                state.complete_bet_or_raise_to(state.stacks[actor] + state.bets[actor])\n",
    "                return \"all_in\", False\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Fallback\n",
    "        try:\n",
    "            state.check_or_call()\n",
    "            return \"call\", True\n",
    "        except:\n",
    "            try:\n",
    "                state.fold()\n",
    "                return \"fold\", True\n",
    "            except:\n",
    "                return \"error\", True\n",
    "\n",
    "    def _get_position_name(self, idx: int) -> str:\n",
    "        pos_map = {2: [\"SB\", \"BB\"], 3: [\"BTN\", \"SB\", \"BB\"], 4: [\"BTN\", \"CO\", \"SB\", \"BB\"]}\n",
    "        positions = pos_map.get(self.num_players, [\"BTN\", \"CO\", \"HJ\", \"LJ\", \"SB\", \"BB\"][:self.num_players])\n",
    "        rel_pos = (idx - self.button) % self.num_players\n",
    "        return positions[rel_pos] if rel_pos < len(positions) else f\"P{idx}\"\n",
    "\n",
    "\n",
    "print(\"EvalPokerGame loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_transformers_model(name: str, model_id: str):\n",
    "    \"\"\"Load a HuggingFace model at full weight (FP16).\"\"\"\n",
    "    print(f\"Loading {name}: {model_id} (FP16 - full weight)...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"  Loaded. VRAM: {allocated:.1f}GB\")\n",
    "\n",
    "    return TransformersPlayer(name, model, tokenizer)\n",
    "\n",
    "\n",
    "def unload_model(player):\n",
    "    \"\"\"Unload model to free VRAM.\"\"\"\n",
    "    if hasattr(player, 'model'):\n",
    "        del player.model\n",
    "        del player.tokenizer\n",
    "    del player\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"  Model unloaded. VRAM: {torch.cuda.memory_allocated() / 1024**3:.1f}GB\")\n",
    "\n",
    "\n",
    "print(\"Model loading functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "# Tournament state\n",
    "matchup_results = []\n",
    "champion = None\n",
    "observability = ObservabilityCollector(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "def run_matchup(p1_name: str, p2_name: str, round_name: str) -> Tuple[str, dict]:\n",
    "    \"\"\"Run a single matchup. Returns (winner_name, result_dict).\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{round_name}: {p1_name} vs {p2_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load players\n",
    "    if MODELS[p1_name][\"type\"] == \"openai\":\n",
    "        p1 = OpenAIPlayer(p1_name, model=MODELS[p1_name].get(\"model\", \"gpt-4\"))\n",
    "    else:\n",
    "        p1 = load_transformers_model(p1_name, MODELS[p1_name][\"model_id\"])\n",
    "\n",
    "    if MODELS[p2_name][\"type\"] == \"openai\":\n",
    "        p2 = OpenAIPlayer(p2_name, model=MODELS[p2_name].get(\"model\", \"gpt-4\"))\n",
    "    else:\n",
    "        p2 = load_transformers_model(p2_name, MODELS[p2_name][\"model_id\"])\n",
    "\n",
    "    # Create game\n",
    "    metrics = MetricsCollector(f\"{round_name}_{p1_name}_vs_{p2_name}\")\n",
    "    pbar = tqdm(total=HANDS_PER_MATCHUP, desc=f\"{p1_name} vs {p2_name}\")\n",
    "\n",
    "    def update_progress(current, total):\n",
    "        pbar.n = current\n",
    "        pbar.refresh()\n",
    "\n",
    "    game = EvalPokerGame(\n",
    "        players=[p1, p2],\n",
    "        starting_stack=STARTING_STACK,\n",
    "        small_blind=SMALL_BLIND,\n",
    "        big_blind=BIG_BLIND,\n",
    "        metrics=metrics,\n",
    "        observability=observability,\n",
    "        verbose=VERBOSE,\n",
    "        progress_callback=update_progress,\n",
    "    )\n",
    "\n",
    "    # Run matchup\n",
    "    result = game.play_session(HANDS_PER_MATCHUP)\n",
    "    pbar.close()\n",
    "\n",
    "    # Write traces\n",
    "    observability.write_traces(f\"{round_name.replace(' ', '_')}\")\n",
    "\n",
    "    # Determine winner\n",
    "    p1_delta = result.player_summaries[p1_name][\"total_chip_delta\"]\n",
    "    p2_delta = result.player_summaries[p2_name][\"total_chip_delta\"]\n",
    "\n",
    "    if p1_delta > p2_delta:\n",
    "        winner = p1_name\n",
    "    elif p2_delta > p1_delta:\n",
    "        winner = p2_name\n",
    "    else:\n",
    "        # Tiebreaker: hands won\n",
    "        p1_wins = result.player_summaries[p1_name][\"hands_won\"]\n",
    "        p2_wins = result.player_summaries[p2_name][\"hands_won\"]\n",
    "        winner = p1_name if p1_wins >= p2_wins else p2_name\n",
    "\n",
    "    # Print result\n",
    "    print(f\"\\n{round_name} Result:\")\n",
    "    print(f\"  {p1_name}: {p1_delta:+} chips (BB/100: {result.player_summaries[p1_name]['bb_per_100']:+.2f})\")\n",
    "    print(f\"  {p2_name}: {p2_delta:+} chips (BB/100: {result.player_summaries[p2_name]['bb_per_100']:+.2f})\")\n",
    "    print(f\"  WINNER: {winner}\")\n",
    "\n",
    "    result_dict = {\n",
    "        \"round\": round_name,\n",
    "        \"player1\": p1_name,\n",
    "        \"player2\": p2_name,\n",
    "        \"player1_chips\": p1_delta,\n",
    "        \"player2_chips\": p2_delta,\n",
    "        \"player1_bb100\": result.player_summaries[p1_name][\"bb_per_100\"],\n",
    "        \"player2_bb100\": result.player_summaries[p2_name][\"bb_per_100\"],\n",
    "        \"winner\": winner,\n",
    "        \"hands_played\": result.total_hands,\n",
    "    }\n",
    "\n",
    "    # Unload models to free VRAM\n",
    "    if MODELS[p1_name][\"type\"] == \"transformers\":\n",
    "        unload_model(p1)\n",
    "    if MODELS[p2_name][\"type\"] == \"transformers\":\n",
    "        unload_model(p2)\n",
    "\n",
    "    return winner, result_dict\n",
    "\n",
    "\n",
    "print(\"Tournament ready to start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the gauntlet tournament\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POKER LLM TOURNAMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Format: Gauntlet (1000 hands per matchup)\")\n",
    "print(f\"Winner: Total chip profit\\n\")\n",
    "\n",
    "# Round 1: Qwen3-Base vs Qwen3-SFT\n",
    "r1_winner, r1_result = run_matchup(\"Qwen3-Base\", \"Qwen3-SFT\", \"Round 1\")\n",
    "matchup_results.append(r1_result)\n",
    "\n",
    "# Round 2: Winner R1 vs Llama3-SFT\n",
    "r2_winner, r2_result = run_matchup(r1_winner, \"Llama3-SFT\", \"Round 2\")\n",
    "matchup_results.append(r2_result)\n",
    "\n",
    "# Round 3: Only if your model beat Llama3\n",
    "if r2_winner != \"Llama3-SFT\" and os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(f\"\\n{r2_winner} beat Llama3-SFT! Proceeding to GPT-4 matchup...\")\n",
    "    r3_winner, r3_result = run_matchup(r2_winner, \"GPT-4\", \"Round 3\")\n",
    "    matchup_results.append(r3_result)\n",
    "    champion = r3_winner\n",
    "elif r2_winner == \"Llama3-SFT\":\n",
    "    print(f\"\\nLlama3-SFT won Round 2. Skipping GPT-4 matchup (cost savings).\")\n",
    "    champion = \"Llama3-SFT\"\n",
    "else:\n",
    "    print(f\"\\nNo OpenAI API key. Skipping GPT-4 matchup.\")\n",
    "    champion = r2_winner\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TOURNAMENT CHAMPION: {champion}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Observability Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Export metrics\n",
    "observability.export_metrics()\n",
    "\n",
    "# Error Rate Summary Table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBSERVABILITY: ERROR RATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "error_rows = []\n",
    "for name, m in observability.metrics.items():\n",
    "    error_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Actions\": m.total_actions,\n",
    "        \"Valid Parse\": m.valid_tag_parses,\n",
    "        \"Regex Fallback\": m.regex_fallback_parses,\n",
    "        \"Default Fallback\": m.default_fallback_parses,\n",
    "        \"Error Rate\": f\"{m.parse_error_rate:.1%}\",\n",
    "        \"Exec Failures\": m.action_execution_failures,\n",
    "    })\n",
    "\n",
    "df_errors = pd.DataFrame(error_rows)\n",
    "print(df_errors.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "df_errors.to_csv(f\"{OUTPUT_DIR}/observability/error_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Action Distribution Chart\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# 1. Parse Error Rate\n",
    "ax = axes[0]\n",
    "models = list(observability.metrics.keys())\n",
    "error_rates = [m.parse_error_rate * 100 for m in observability.metrics.values()]\n",
    "colors = [\"green\" if r < 5 else \"orange\" if r < 15 else \"red\" for r in error_rates]\n",
    "ax.bar(models, error_rates, color=colors)\n",
    "ax.set_title(\"Parse Error Rate\", fontsize=14)\n",
    "ax.set_ylabel(\"Error Rate (%)\")\n",
    "ax.set_ylim(0, max(error_rates) * 1.2 if error_rates else 10)\n",
    "\n",
    "# 2. Action Distribution\n",
    "ax = axes[1]\n",
    "action_data = {}\n",
    "for name, m in observability.metrics.items():\n",
    "    total = m.total_actions or 1\n",
    "    action_data[name] = {\n",
    "        \"Fold\": m.fold_count / total * 100,\n",
    "        \"Check\": m.check_count / total * 100,\n",
    "        \"Call\": m.call_count / total * 100,\n",
    "        \"Raise\": m.raise_count / total * 100,\n",
    "        \"All-in\": m.all_in_count / total * 100,\n",
    "    }\n",
    "\n",
    "df_actions = pd.DataFrame(action_data).T\n",
    "df_actions.plot(kind=\"bar\", stacked=True, ax=ax, colormap=\"Set3\")\n",
    "ax.set_title(\"Action Distribution\", fontsize=14)\n",
    "ax.set_ylabel(\"%\")\n",
    "ax.legend(loc=\"upper right\", fontsize=8)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "# 3. Latency\n",
    "ax = axes[2]\n",
    "latency_data = {name: m.latencies for name, m in observability.metrics.items()}\n",
    "ax.boxplot(latency_data.values(), labels=latency_data.keys())\n",
    "ax.set_title(\"Latency Distribution\", fontsize=14)\n",
    "ax.set_ylabel(\"Latency (ms)\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/charts/observability.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tournament Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matchup Results Table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOURNAMENT RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_matchups = pd.DataFrame(matchup_results)\n",
    "print(df_matchups[[\"round\", \"player1\", \"player2\", \"player1_chips\", \"player2_chips\", \"winner\"]].to_string(index=False))\n",
    "\n",
    "# Save\n",
    "df_matchups.to_csv(f\"{OUTPUT_DIR}/matchups.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Chip progression per matchup\n",
    "ax = axes[0]\n",
    "for i, r in enumerate(matchup_results):\n",
    "    x = [i, i]\n",
    "    y = [r[\"player1_chips\"], r[\"player2_chips\"]]\n",
    "    colors = [\"green\" if c > 0 else \"red\" for c in y]\n",
    "    ax.bar([f\"{r['player1']}\\n({r['round']})\", f\"{r['player2']}\\n({r['round']})\"],\n",
    "           [r[\"player1_chips\"], r[\"player2_chips\"]], color=colors, alpha=0.7)\n",
    "\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_title(\"Chip Results by Matchup\", fontsize=14)\n",
    "ax.set_ylabel(\"Chip Delta\")\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=9)\n",
    "\n",
    "# BB/100 comparison\n",
    "ax = axes[1]\n",
    "all_players = set()\n",
    "player_bb100 = {}\n",
    "for r in matchup_results:\n",
    "    if r[\"player1\"] not in player_bb100:\n",
    "        player_bb100[r[\"player1\"]] = []\n",
    "    if r[\"player2\"] not in player_bb100:\n",
    "        player_bb100[r[\"player2\"]] = []\n",
    "    player_bb100[r[\"player1\"]].append(r[\"player1_bb100\"])\n",
    "    player_bb100[r[\"player2\"]].append(r[\"player2_bb100\"])\n",
    "\n",
    "avg_bb100 = {p: sum(v)/len(v) for p, v in player_bb100.items()}\n",
    "colors = [\"green\" if v > 0 else \"red\" for v in avg_bb100.values()]\n",
    "ax.bar(avg_bb100.keys(), avg_bb100.values(), color=colors)\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_title(\"Average BB/100 by Model\", fontsize=14)\n",
    "ax.set_ylabel(\"BB/100\")\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/charts/tournament_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Blog-Ready Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BLOG_SUMMARY.md\n",
    "blog_md = f\"\"\"# Poker LLM Tournament Results\n",
    "\n",
    "## Champion: {champion}\n",
    "\n",
    "### Tournament Bracket\n",
    "| Round | Matchup | Winner | Chip Differential |\n",
    "|-------|---------|--------|-------------------|\n",
    "\"\"\"\n",
    "\n",
    "for r in matchup_results:\n",
    "    winner_chips = r[\"player1_chips\"] if r[\"winner\"] == r[\"player1\"] else r[\"player2_chips\"]\n",
    "    blog_md += f\"| {r['round']} | {r['player1']} vs {r['player2']} | {r['winner']} | {winner_chips:+} |\\n\"\n",
    "\n",
    "blog_md += f\"\"\"\n",
    "### Key Statistics\n",
    "- **Total hands played**: {sum(r['hands_played'] for r in matchup_results)}\n",
    "- **Matchups completed**: {len(matchup_results)}\n",
    "\"\"\"\n",
    "\n",
    "# Add model comparison\n",
    "blog_md += \"\\n### Model Performance\\n\"\n",
    "blog_md += \"| Model | Avg BB/100 | Error Rate |\\n\"\n",
    "blog_md += \"|-------|------------|------------|\\n\"\n",
    "for name, bb in avg_bb100.items():\n",
    "    err = observability.metrics.get(name)\n",
    "    err_rate = f\"{err.parse_error_rate:.1%}\" if err else \"N/A\"\n",
    "    blog_md += f\"| {name} | {bb:+.2f} | {err_rate} |\\n\"\n",
    "\n",
    "blog_md += f\"\"\"\n",
    "### Notable Findings\n",
    "- Champion **{champion}** emerged victorious after {len(matchup_results)} rounds\n",
    "\"\"\"\n",
    "\n",
    "if \"Llama3-SFT\" in [r[\"winner\"] for r in matchup_results if r[\"round\"] == \"Round 2\"]:\n",
    "    blog_md += \"- The PokerBench paper model (Llama3-SFT) proved superior to custom fine-tunes\\n\"\n",
    "else:\n",
    "    blog_md += f\"- Custom fine-tuned model beat the PokerBench benchmark (Llama3-SFT)\\n\"\n",
    "\n",
    "# Write file\n",
    "with open(f\"{OUTPUT_DIR}/BLOG_SUMMARY.md\", \"w\") as f:\n",
    "    f.write(blog_md)\n",
    "\n",
    "print(blog_md)\n",
    "print(f\"\\nSaved to: {OUTPUT_DIR}/BLOG_SUMMARY.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quotable Stats (for easy copy-paste)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUOTABLE STATS (copy-paste ready)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Champion: {champion}\")\n",
    "\n",
    "if len(matchup_results) > 0:\n",
    "    final = matchup_results[-1]\n",
    "    margin = abs(final[\"player1_chips\"] - final[\"player2_chips\"])\n",
    "    print(f\"Final margin: {margin:,} chips ({margin // BIG_BLIND} BB)\")\n",
    "\n",
    "# Best local model\n",
    "local_models = [\"Qwen3-Base\", \"Qwen3-SFT\"]\n",
    "local_bb = {m: avg_bb100.get(m, 0) for m in local_models if m in avg_bb100}\n",
    "if local_bb:\n",
    "    best_local = max(local_bb, key=local_bb.get)\n",
    "    print(f\"Best local model: {best_local} (BB/100: {local_bb[best_local]:+.2f})\")\n",
    "\n",
    "# Error rates\n",
    "print(\"\\nError rates:\")\n",
    "for name, m in sorted(observability.metrics.items(), key=lambda x: x[1].parse_error_rate):\n",
    "    print(f\"  {name}: {m.parse_error_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Save tournament.json\n",
    "tournament_data = {\n",
    "    \"champion\": champion,\n",
    "    \"config\": {\n",
    "        \"hands_per_matchup\": HANDS_PER_MATCHUP,\n",
    "        \"starting_stack\": STARTING_STACK,\n",
    "        \"blinds\": f\"{SMALL_BLIND}/{BIG_BLIND}\",\n",
    "        \"gpu\": GPU_NAME,\n",
    "    },\n",
    "    \"matchups\": matchup_results,\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/tournament.json\", \"w\") as f:\n",
    "    json.dump(tournament_data, f, indent=2)\n",
    "\n",
    "# Copy to Drive\n",
    "drive_path = \"/content/drive/MyDrive/poker_tournament_results\"\n",
    "shutil.copytree(OUTPUT_DIR, drive_path, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"Results exported to Google Drive: {drive_path}/\")\n",
    "print(f\"\\nFiles:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    print(f\"  - {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
