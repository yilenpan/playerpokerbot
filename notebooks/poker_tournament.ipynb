{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poker LLM Tournament\n",
    "\n",
    "Gauntlet-style tournament comparing poker LLMs.\n",
    "\n",
    "**Quick Start:** Run all cells. Results saved to `/content/tournament_results/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -q transformers accelerate bitsandbytes torch pokerkit tqdm pandas matplotlib openai\n",
    "!git clone https://github.com/yilenpan/player_poker_bot.git /content/player_poker_bot 2>/dev/null || true\n",
    "!pip install -q -e /content/player_poker_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key from Colab secrets\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "try:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    print(\"OpenAI API key loaded\")\n",
    "except:\n",
    "    print(\"No OpenAI API key found - GPT-4 matchups will be skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from src.eval import (\n",
    "    HardwareConfig,\n",
    "    TransformersPlayer,\n",
    "    OpenAIPlayer,\n",
    "    MetricsCollector,\n",
    "    EvalPokerGame,\n",
    "    ObservabilityCollector,\n",
    ")\n",
    "\n",
    "print(\"Imports loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware detection\n",
    "hw = HardwareConfig.detect()\n",
    "print(f\"GPU: {hw.gpu_name} ({hw.vram_gb:.0f}GB), Quantization: {hw.quantization.value}\")\n",
    "\n",
    "# Tournament settings\n",
    "DRY_RUN = True  # False for full 1000-hand matchups\n",
    "HANDS_PER_MATCHUP = 10 if DRY_RUN else 1000\n",
    "\n",
    "STARTING_STACK = 10000\n",
    "SMALL_BLIND = 50\n",
    "BIG_BLIND = 100\n",
    "SEED = 42\n",
    "\n",
    "# Models\n",
    "MODELS = {\n",
    "    \"Qwen3-Base\": {\"type\": \"transformers\", \"model_id\": \"unsloth/Qwen3-4B-Thinking-2507\"},\n",
    "    \"Qwen3-SFT\": {\"type\": \"transformers\", \"model_id\": \"YiPz/qwen3-4b-pokergpt-o3-sft-lora\"},\n",
    "    \"Llama3-SFT\": {\"type\": \"transformers\", \"model_id\": \"YiPz/llama3-8b-pokerbench-sft\"},\n",
    "    \"GPT-4\": {\"type\": \"openai\", \"model\": \"gpt-4\"},\n",
    "}\n",
    "\n",
    "# Gauntlet bracket\n",
    "GAUNTLET = [\n",
    "    (\"Qwen3-Base\", \"Qwen3-SFT\"),   # R1: Your models\n",
    "    (\"WINNER_R1\", \"Llama3-SFT\"),    # R2: vs Benchmark\n",
    "    (\"WINNER_R2\", \"GPT-4\"),         # R3: vs GPT-4\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"/content/tournament_results\"\n",
    "os.makedirs(f\"{OUTPUT_DIR}/charts\", exist_ok=True)\n",
    "\n",
    "print(f\"Mode: {'DRY RUN' if DRY_RUN else 'FULL'}, Hands: {HANDS_PER_MATCHUP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_cache = {}\n",
    "\n",
    "def load_transformers_model(name: str, model_id: str) -> TransformersPlayer:\n",
    "    \"\"\"Load a transformers model, caching tokenizer.\"\"\"\n",
    "    if name in loaded_cache:\n",
    "        model, tokenizer = loaded_cache[name]\n",
    "    else:\n",
    "        print(f\"  Loading {name}: {model_id}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "        \n",
    "        load_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True, \"torch_dtype\": torch.float16}\n",
    "        bnb_config = hw.get_bnb_config()\n",
    "        if bnb_config:\n",
    "            load_kwargs[\"quantization_config\"] = bnb_config\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, **load_kwargs)\n",
    "        loaded_cache[name] = (model, tokenizer)\n",
    "        print(f\"  VRAM: {torch.cuda.memory_allocated() / 1024**3:.1f}GB\")\n",
    "    \n",
    "    return TransformersPlayer(name, loaded_cache[name][0], loaded_cache[name][1])\n",
    "\n",
    "\n",
    "def unload_model(name: str):\n",
    "    \"\"\"Unload a model to free VRAM.\"\"\"\n",
    "    if name in loaded_cache:\n",
    "        del loaded_cache[name]\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"  Unloaded {name}, VRAM: {torch.cuda.memory_allocated() / 1024**3:.1f}GB\")\n",
    "\n",
    "\n",
    "print(\"Helpers ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "matchup_results = []\n",
    "observability = ObservabilityCollector(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "def run_matchup(p1_name: str, p2_name: str, round_name: str) -> Tuple[str, dict]:\n",
    "    \"\"\"Run a single matchup. Returns (winner_name, result_dict).\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{round_name}: {p1_name} vs {p2_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load players\n",
    "    if MODELS[p1_name][\"type\"] == \"openai\":\n",
    "        p1 = OpenAIPlayer(p1_name, model=MODELS[p1_name].get(\"model\", \"gpt-4\"))\n",
    "    else:\n",
    "        p1 = load_transformers_model(p1_name, MODELS[p1_name][\"model_id\"])\n",
    "\n",
    "    if MODELS[p2_name][\"type\"] == \"openai\":\n",
    "        p2 = OpenAIPlayer(p2_name, model=MODELS[p2_name].get(\"model\", \"gpt-4\"))\n",
    "    else:\n",
    "        p2 = load_transformers_model(p2_name, MODELS[p2_name][\"model_id\"])\n",
    "\n",
    "    # Create game\n",
    "    metrics = MetricsCollector(f\"{round_name}_{p1_name}_vs_{p2_name}\")\n",
    "    pbar = tqdm(total=HANDS_PER_MATCHUP, desc=f\"{p1_name} vs {p2_name}\")\n",
    "\n",
    "    game = EvalPokerGame(\n",
    "        players=[p1, p2],\n",
    "        starting_stack=STARTING_STACK,\n",
    "        small_blind=SMALL_BLIND,\n",
    "        big_blind=BIG_BLIND,\n",
    "        metrics=metrics,\n",
    "        observability=observability,\n",
    "        progress_callback=lambda cur, tot: (setattr(pbar, 'n', cur), pbar.refresh()),\n",
    "    )\n",
    "\n",
    "    result = game.play_session(HANDS_PER_MATCHUP)\n",
    "    pbar.close()\n",
    "\n",
    "    # Determine winner\n",
    "    p1_delta = result.player_summaries[p1_name][\"total_chip_delta\"]\n",
    "    p2_delta = result.player_summaries[p2_name][\"total_chip_delta\"]\n",
    "    winner = p1_name if p1_delta >= p2_delta else p2_name\n",
    "\n",
    "    print(f\"\\nResult: {p1_name} {p1_delta:+} | {p2_name} {p2_delta:+} | WINNER: {winner}\")\n",
    "\n",
    "    result_dict = {\n",
    "        \"round\": round_name,\n",
    "        \"player1\": p1_name, \"player2\": p2_name,\n",
    "        \"player1_chips\": p1_delta, \"player2_chips\": p2_delta,\n",
    "        \"player1_bb100\": result.player_summaries[p1_name][\"bb_per_100\"],\n",
    "        \"player2_bb100\": result.player_summaries[p2_name][\"bb_per_100\"],\n",
    "        \"winner\": winner,\n",
    "        \"hands_played\": result.total_hands,\n",
    "    }\n",
    "\n",
    "    # Unload losing model\n",
    "    loser = p2_name if winner == p1_name else p1_name\n",
    "    if MODELS[loser][\"type\"] == \"transformers\":\n",
    "        unload_model(loser)\n",
    "\n",
    "    return winner, result_dict\n",
    "\n",
    "\n",
    "print(\"Tournament ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gauntlet\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POKER LLM TOURNAMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "champion = None\n",
    "\n",
    "for i, (p1_template, p2_name) in enumerate(GAUNTLET):\n",
    "    round_name = f\"Round {i+1}\"\n",
    "    \n",
    "    # Resolve winner placeholders\n",
    "    p1_name = champion if p1_template.startswith(\"WINNER\") else p1_template\n",
    "    \n",
    "    # Skip if GPT-4 and no API key\n",
    "    if p2_name == \"GPT-4\" and not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "        print(f\"\\nSkipping {round_name}: No OpenAI API key\")\n",
    "        break\n",
    "    \n",
    "    # Skip if SFT lost to Base\n",
    "    if i == 1 and champion == \"Qwen3-Base\":\n",
    "        print(f\"\\nStopping: Base model beat SFT - training needs work!\")\n",
    "        break\n",
    "    \n",
    "    champion, result = run_matchup(p1_name, p2_name, round_name)\n",
    "    matchup_results.append(result)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CHAMPION: {champion}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "df = pd.DataFrame(matchup_results)\n",
    "print(df[[\"round\", \"player1\", \"player2\", \"player1_bb100\", \"player2_bb100\", \"winner\"]].to_string(index=False))\n",
    "\n",
    "# Observability summary\n",
    "observability.export_metrics()\n",
    "print(f\"\\nAction parse errors: {observability.total_errors()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "if len(matchup_results) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    for i, r in enumerate(matchup_results):\n",
    "        ax.barh(i - 0.2, r[\"player1_bb100\"], 0.35, label=r[\"player1\"] if i == 0 else \"\", color=\"steelblue\")\n",
    "        ax.barh(i + 0.2, r[\"player2_bb100\"], 0.35, label=r[\"player2\"] if i == 0 else \"\", color=\"coral\")\n",
    "        ax.text(r[\"player1_bb100\"], i - 0.2, f\" {r['player1']}\", va='center', fontsize=9)\n",
    "        ax.text(r[\"player2_bb100\"], i + 0.2, f\" {r['player2']}\", va='center', fontsize=9)\n",
    "    \n",
    "    ax.axvline(x=0, color=\"black\", linewidth=0.5)\n",
    "    ax.set_yticks(range(len(matchup_results)))\n",
    "    ax.set_yticklabels([r[\"round\"] for r in matchup_results])\n",
    "    ax.set_xlabel(\"BB/100\")\n",
    "    ax.set_title(\"Tournament Results\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/charts/tournament.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "tournament_data = {\n",
    "    \"champion\": champion,\n",
    "    \"config\": {\"hands_per_matchup\": HANDS_PER_MATCHUP, \"dry_run\": DRY_RUN},\n",
    "    \"matchups\": matchup_results,\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/tournament.json\", \"w\") as f:\n",
    "    json.dump(tournament_data, f, indent=2)\n",
    "\n",
    "df.to_csv(f\"{OUTPUT_DIR}/results.csv\", index=False)\n",
    "\n",
    "# Blog summary\n",
    "blog_md = f\"\"\"# Poker LLM Tournament Results\n",
    "\n",
    "## Champion: {champion}\n",
    "\n",
    "| Round | Player 1 | Player 2 | Winner |\n",
    "|-------|----------|----------|--------|\n",
    "\"\"\"\n",
    "for r in matchup_results:\n",
    "    blog_md += f\"| {r['round']} | {r['player1']} ({r['player1_bb100']:+.1f}) | {r['player2']} ({r['player2_bb100']:+.1f}) | {r['winner']} |\\n\"\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/BLOG_SUMMARY.md\", \"w\") as f:\n",
    "    f.write(blog_md)\n",
    "\n",
    "print(f\"Saved to {OUTPUT_DIR}/: tournament.json, results.csv, BLOG_SUMMARY.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
