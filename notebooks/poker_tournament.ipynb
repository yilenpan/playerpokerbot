{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poker LLM Tournament\n",
    "\n",
    "Single-elimination tournament comparing poker LLMs with **1000 hands per matchup**.\n",
    "\n",
    "## Models\n",
    "| Model | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| Qwen3-Base | HuggingFace | `unsloth/Qwen3-4B-Thinking-2507` - Base thinking model |\n",
    "| Qwen3-SFT | HuggingFace | `YiPz/qwen3-4b-pokergpt-o3-sft-lora` - Fine-tuned on 5k hands |\n",
    "| Llama3-SFT | HuggingFace | `YiPz/llama3-8b-pokerbench-sft` - From PokerBench paper |\n",
    "| GPT-4 | OpenAI API | Only runs if your model beats Llama3 |\n",
    "\n",
    "## Gauntlet Format (Cost Optimized)\n",
    "```\n",
    "Round 1: Qwen3-Base vs Qwen3-SFT     (your models head-to-head)\n",
    "Round 2: Winner R1 vs Llama3-SFT     (benchmark test)\n",
    "Round 3: Winner R2 vs GPT-4          (only if your model wins R2)\n",
    "```\n",
    "\n",
    "**Winner determined by total chip profit after 1000 hands.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for model caching\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers accelerate torch pokerkit\n",
    "!pip install -q tqdm pandas matplotlib openai\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/content/drive/MyDrive/hf_cache\"\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key from Colab secrets\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    print(\"OpenAI API key loaded from secrets\")\n",
    "except:\n",
    "    print(\"Warning: OPENAI_API_KEY not found in secrets. GPT-4 matchup will be skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "\n",
    "def detect_gpu():\n",
    "    \"\"\"Detect GPU and VRAM.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        gpu_name, vram_mb = result.stdout.strip().split(\", \")\n",
    "        vram_gb = float(vram_mb) / 1024\n",
    "    except:\n",
    "        gpu_name, vram_gb = \"Unknown\", 16.0\n",
    "    return gpu_name, vram_gb\n",
    "\n",
    "GPU_NAME, VRAM_GB = detect_gpu()\n",
    "print(f\"GPU: {GPU_NAME} ({VRAM_GB:.0f}GB)\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check if we have enough VRAM for full weight Llama3-8B (~16GB)\n",
    "if VRAM_GB < 20:\n",
    "    print(f\"\\nWarning: Llama3-8B requires ~16GB VRAM at FP16.\")\n",
    "    print(\"Models will be loaded/unloaded sequentially to manage memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tournament Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tournament settings\n",
    "HANDS_PER_MATCHUP = 1000\n",
    "STARTING_STACK = 10000\n",
    "SMALL_BLIND = 50\n",
    "BIG_BLIND = 100\n",
    "SEED = 42\n",
    "VERBOSE = False  # Set True to see each action\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"Qwen3-Base\": {\n",
    "        \"type\": \"transformers\",\n",
    "        \"model_id\": \"unsloth/Qwen3-4B-Thinking-2507\",\n",
    "    },\n",
    "    \"Qwen3-SFT\": {\n",
    "        \"type\": \"transformers\",\n",
    "        \"model_id\": \"YiPz/qwen3-4b-pokergpt-o3-sft-lora\",\n",
    "    },\n",
    "    \"Llama3-SFT\": {\n",
    "        \"type\": \"transformers\",\n",
    "        \"model_id\": \"YiPz/llama3-8b-pokerbench-sft\",\n",
    "    },\n",
    "    \"GPT-4\": {\n",
    "        \"type\": \"openai\",\n",
    "        \"model\": \"gpt-4\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Gauntlet order\n",
    "GAUNTLET = [\n",
    "    (\"Qwen3-Base\", \"Qwen3-SFT\"),   # Round 1: Your models\n",
    "    (\"WINNER_R1\", \"Llama3-SFT\"),    # Round 2: vs Benchmark\n",
    "    (\"WINNER_R2\", \"GPT-4\"),         # Round 3: vs GPT-4 (conditional)\n",
    "]\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"/content/tournament_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/observability/traces\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/charts\", exist_ok=True)\n",
    "\n",
    "print(f\"Tournament Config:\")\n",
    "print(f\"  Hands per matchup: {HANDS_PER_MATCHUP}\")\n",
    "print(f\"  Stack: {STARTING_STACK}\")\n",
    "print(f\"  Blinds: {SMALL_BLIND}/{BIG_BLIND}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport random\nimport json\nimport re\nimport gc\nfrom dataclasses import dataclass, field, asdict\nfrom typing import List, Tuple, Dict, Any, Optional\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom pokerkit import NoLimitTexasHoldem, Automation\n\n\n# ============= Card & Position Utilities =============\n\nSUIT_MAP = {\"s\": \"♠\", \"h\": \"♥\", \"d\": \"♦\", \"c\": \"♣\"}\nRANK_ORDER = \"23456789TJQKA\"\nRANK_VALUE = {r: i for i, r in enumerate(RANK_ORDER, start=2)}\n\n\ndef pretty_card(card: str) -> str:\n    \"\"\"Format a card string with pretty suit symbols. 'As' -> 'A♠'\"\"\"\n    if len(card) < 2:\n        return card\n    rank = card[:-1]\n    suit = SUIT_MAP.get(card[-1].lower(), card[-1])\n    return f\"{rank}{suit}\"\n\n\ndef score_hole_cards(c1: str, c2: str) -> int:\n    \"\"\"Score preflop hole cards (0-128, where 128 = AA).\"\"\"\n    r1 = c1[0].upper() if c1 else \"2\"\n    r2 = c2[0].upper() if c2 else \"2\"\n    v1 = RANK_VALUE.get(r1, 2)\n    v2 = RANK_VALUE.get(r2, 2)\n    high, low = max(v1, v2), min(v1, v2)\n    is_pair = v1 == v2\n    is_suited = len(c1) > 1 and len(c2) > 1 and c1[-1].lower() == c2[-1].lower()\n\n    if is_pair:\n        return 100 + high * 2\n\n    score = high * 4 + low\n    if is_suited:\n        score += 12\n    gap = high - low\n    if gap == 1:\n        score += 6\n    elif gap == 2:\n        score += 3\n    if high >= 11 and low >= 10:\n        score += 6\n    if high == 14:\n        score += 4\n    return score\n\n\ndef get_position_name(player_idx: int, num_players: int, button_idx: int) -> str:\n    \"\"\"Get position name (UTG, MP, CO, BTN, SB, BB).\"\"\"\n    if num_players < 2:\n        return \"Unknown\"\n    offset = (player_idx - button_idx) % num_players\n\n    if num_players == 2:\n        return \"BTN/SB\" if offset == 0 else \"BB\"\n    if offset == 0:\n        return \"BTN\"\n    elif offset == 1:\n        return \"SB\"\n    elif offset == 2:\n        return \"BB\"\n\n    if num_players == 4:\n        if offset == 3: return \"CO\"\n    elif num_players == 5:\n        if offset == 3: return \"MP\"\n        elif offset == 4: return \"CO\"\n    elif num_players == 6:\n        if offset == 3: return \"UTG\"\n        elif offset == 4: return \"MP\"\n        elif offset == 5: return \"CO\"\n    elif num_players >= 7:\n        positions = [\"BTN\", \"SB\", \"BB\", \"UTG\", \"UTG+1\", \"MP\", \"MP+1\", \"HJ\", \"CO\"]\n        if offset < len(positions):\n            return positions[offset]\n\n    return f\"P{player_idx + 1}\"\n\n\n# ============= Action Parsing =============\n\n@dataclass\nclass ParsedAction:\n    action_type: str\n    amount: Optional[int] = None\n\n    def __str__(self):\n        if self.amount:\n            return f\"{self.action_type.title()} {self.amount}\"\n        return self.action_type.title()\n\n\n@dataclass\nclass ParseResult:\n    action: ParsedAction\n    method: str  # \"tag\" | \"regex_*\" | \"default\"\n    raw_match: str\n    error: Optional[str] = None\n\n\nclass ActionParser:\n    RE_ACTION_TAG = re.compile(r\"<action>\\s*([^<]+?)\\s*</action>\", re.IGNORECASE)\n    RE_FOLD = re.compile(r\"\\b(f|fold)\\b\", re.IGNORECASE)\n    RE_CC = re.compile(r\"\\b(cc|call|check)\\b\", re.IGNORECASE)\n    RE_CBR = re.compile(r\"\\b(?:cbr|bet|raise)(?:\\s+(?:to\\s+)?(\\d+))?\\b\", re.IGNORECASE)\n    RE_ALL_IN = re.compile(r\"\\b(all[\\-\\s]?in|shove)\\b\", re.IGNORECASE)\n\n    def parse(self, text: str, can_check: bool = True, stack: int = 0) -> ParsedAction:\n        return self.parse_with_metadata(text, can_check, stack).action\n\n    def parse_with_metadata(self, text: str, can_check: bool = True, stack: int = 0) -> ParseResult:\n        tag_match = self.RE_ACTION_TAG.search(text)\n        used_tag = tag_match is not None\n        content = tag_match.group(1).strip() if tag_match else text\n\n        if self.RE_ALL_IN.search(content):\n            return ParseResult(ParsedAction(\"all_in\", stack), \"tag\" if used_tag else \"regex_allin\", content)\n        if self.RE_FOLD.search(content):\n            return ParseResult(ParsedAction(\"fold\"), \"tag\" if used_tag else \"regex_fold\", content)\n        if self.RE_CC.search(content):\n            action = ParsedAction(\"check\" if can_check else \"call\")\n            return ParseResult(action, \"tag\" if used_tag else \"regex_call\", content)\n\n        cbr = self.RE_CBR.search(content)\n        if cbr:\n            amt = int(cbr.group(1)) if cbr.group(1) else stack\n            return ParseResult(ParsedAction(\"raise\", amt), \"tag\" if used_tag else \"regex_raise\", content)\n\n        default_action = ParsedAction(\"check\" if can_check else \"fold\")\n        return ParseResult(default_action, \"default\", content[:100], \"No valid action pattern found\")\n\n\n# ============= Action Record =============\n\n@dataclass\nclass ActionRecord:\n    hand_id: int\n    street: str\n    hole_cards: Tuple[str, str]\n    board: List[str]\n    pot: int\n    to_call: int\n    stack: int\n    position: str\n    action: ParsedAction\n    thinking: str\n    response: str\n    latency_ms: float\n    tokens_generated: int\n    parse_method: str = \"unknown\"\n    parse_error: Optional[str] = None\n\n\n# ============= PromptBuilder (pokergpt format) =============\n\nclass PromptBuilder:\n    \"\"\"Builds prompts in pokergpt format for LLM poker players.\"\"\"\n\n    def __init__(self, big_blind: int = 100):\n        self.big_blind = big_blind\n        self.action_history: List[str] = []\n\n    def record_deal(self, player_label: str, is_hero: bool = False, blind_note: str = \"\"):\n        \"\"\"Record a hole card deal.\"\"\"\n        suffix = f\" ({blind_note})\" if blind_note else \"\"\n        if is_hero:\n            self.action_history.append(f\"{player_label} were dealt your hole cards{suffix}.\")\n        else:\n            self.action_history.append(f\"{player_label} was dealt hole cards{suffix}.\")\n\n    def record_board(self, board_cards: List[str]):\n        \"\"\"Record a board deal.\"\"\"\n        n = len(board_cards)\n        if n == 3:\n            street = \"Flop\"\n        elif n == 4:\n            street = \"Turn\"\n        elif n == 5:\n            street = \"River\"\n        else:\n            street = \"Board\"\n        pretty = \" \".join(pretty_card(c) for c in board_cards)\n        self.action_history.append(f\"{street} dealt: {pretty}\")\n\n    def record_action(self, player_label: str, action: str, amount_bb: Optional[float] = None):\n        \"\"\"Record a player action.\"\"\"\n        if amount_bb is not None:\n            self.action_history.append(f\"{player_label} {action} {amount_bb:.1f} BB.\")\n        else:\n            self.action_history.append(f\"{player_label} {action}.\")\n\n    def reset_hand(self):\n        \"\"\"Reset action history for a new hand.\"\"\"\n        self.action_history = []\n\n    def get_player_label(self, player_idx: int, hero_idx: int, positions: List[str]) -> str:\n        \"\"\"Get label for a player.\"\"\"\n        pos = positions[player_idx] if player_idx < len(positions) else f\"P{player_idx + 1}\"\n        if player_idx == hero_idx:\n            return f\"You ({pos})\"\n        return pos\n\n    def build_prompt(\n        self,\n        hero_idx: int,\n        hero_cards: Tuple[str, str],\n        board: List[str],\n        stacks: List[int],\n        bets: List[int],\n        pot: int,\n        to_call: int,\n        min_raise: int,\n        button_idx: int,\n        num_players: int,\n        street: str = \"preflop\",\n    ) -> str:\n        \"\"\"Build a prompt in pokergpt format.\"\"\"\n        bb = self.big_blind\n        positions = [get_position_name(i, num_players, button_idx) for i in range(num_players)]\n        hero_pos = positions[hero_idx]\n\n        lines = [\n            \"You are an expert poker player and you are playing NT poker.\",\n            f\"There are {num_players} players at the table.\",\n            f\"You are in the {hero_pos} position.\",\n            \"\",\n            \"Stacks:\",\n        ]\n\n        for i, stack in enumerate(stacks):\n            label = self.get_player_label(i, hero_idx, positions)\n            lines.append(f\"- {label}: {stack / bb:.1f} BB\")\n\n        if self.action_history:\n            lines.extend([\"\", \"Actions so far:\"])\n            for action in self.action_history:\n                lines.append(f\"- {action}\")\n\n        c1, c2 = hero_cards\n        lines.extend([\n            \"\",\n            f\"Your hole cards are: {pretty_card(c1)} {pretty_card(c2)}\",\n        ])\n\n        if street == \"preflop\":\n            strength = score_hole_cards(c1, c2)\n            lines.append(f\"Preflop hand strength score out of 128 (128 is pair Aces): {strength}\")\n        elif board:\n            pretty_board = \" \".join(pretty_card(c) for c in board)\n            lines.append(f\"The current board is: {pretty_board}\")\n\n        if any(b > 0 for b in bets):\n            lines.extend([\"\", \"The current bets are:\"])\n            for i, bet in enumerate(bets):\n                if bet > 0:\n                    label = self.get_player_label(i, hero_idx, positions)\n                    lines.append(f\"- {label}: {bet / bb:.1f} BB\")\n\n        lines.extend([\n            \"\",\n            f\"The current pot size is: {pot / bb:.1f} BB\",\n            \"It is now your turn to act.\",\n            \"Minimum bet: 1 BB.\",\n            \"\",\n            \"Available actions:\",\n        ])\n\n        if to_call > 0:\n            lines.append(\"- Fold\")\n            lines.append(f\"- Call {to_call / bb:.0f} BB\")\n            lines.append(f\"- Raise (minimum: {min_raise / bb:.0f} BB)\")\n        else:\n            lines.append(\"- Check\")\n            lines.append(\"- Bet (minimum: 1 BB)\")\n\n        return \"\\n\".join(lines)\n\n\nprint(\"Core classes loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Observability =============\n",
    "\n",
    "@dataclass\n",
    "class ModelObservability:\n",
    "    model_name: str\n",
    "    total_actions: int = 0\n",
    "    valid_tag_parses: int = 0\n",
    "    regex_fallback_parses: int = 0\n",
    "    default_fallback_parses: int = 0\n",
    "    action_execution_failures: int = 0\n",
    "    empty_responses: int = 0\n",
    "    fold_count: int = 0\n",
    "    check_count: int = 0\n",
    "    call_count: int = 0\n",
    "    raise_count: int = 0\n",
    "    all_in_count: int = 0\n",
    "    latencies: List[float] = field(default_factory=list)\n",
    "    total_tokens: int = 0\n",
    "\n",
    "    @property\n",
    "    def parse_error_rate(self) -> float:\n",
    "        if self.total_actions == 0:\n",
    "            return 0.0\n",
    "        return (self.regex_fallback_parses + self.default_fallback_parses) / self.total_actions\n",
    "\n",
    "    @property\n",
    "    def avg_latency_ms(self) -> float:\n",
    "        return sum(self.latencies) / len(self.latencies) if self.latencies else 0.0\n",
    "\n",
    "    @property\n",
    "    def p99_latency_ms(self) -> float:\n",
    "        if not self.latencies:\n",
    "            return 0.0\n",
    "        sorted_lat = sorted(self.latencies)\n",
    "        return sorted_lat[int(len(sorted_lat) * 0.99)]\n",
    "\n",
    "\n",
    "class ObservabilityCollector:\n",
    "    def __init__(self, output_dir: str):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.traces: Dict[str, List[dict]] = {}\n",
    "        self.metrics: Dict[str, ModelObservability] = {}\n",
    "\n",
    "    def record_action(self, model_name: str, record: ActionRecord, executed_action: str, fallback_used: bool):\n",
    "        # Store trace\n",
    "        if model_name not in self.traces:\n",
    "            self.traces[model_name] = []\n",
    "\n",
    "        trace = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"hand_id\": record.hand_id,\n",
    "            \"street\": record.street,\n",
    "            \"hole_cards\": list(record.hole_cards),\n",
    "            \"board\": record.board,\n",
    "            \"pot\": record.pot,\n",
    "            \"to_call\": record.to_call,\n",
    "            \"stack\": record.stack,\n",
    "            \"position\": record.position,\n",
    "            \"raw_response\": record.response,\n",
    "            \"thinking\": record.thinking,\n",
    "            \"parsed_action\": record.action.action_type,\n",
    "            \"parsed_amount\": record.action.amount,\n",
    "            \"parse_method\": record.parse_method,\n",
    "            \"parse_error\": record.parse_error,\n",
    "            \"executed_action\": executed_action,\n",
    "            \"fallback_used\": fallback_used,\n",
    "            \"latency_ms\": record.latency_ms,\n",
    "            \"tokens\": record.tokens_generated,\n",
    "        }\n",
    "        self.traces[model_name].append(trace)\n",
    "\n",
    "        # Update metrics\n",
    "        if model_name not in self.metrics:\n",
    "            self.metrics[model_name] = ModelObservability(model_name=model_name)\n",
    "        m = self.metrics[model_name]\n",
    "\n",
    "        m.total_actions += 1\n",
    "        m.latencies.append(record.latency_ms)\n",
    "        m.total_tokens += record.tokens_generated\n",
    "\n",
    "        if record.parse_method == \"tag\":\n",
    "            m.valid_tag_parses += 1\n",
    "        elif record.parse_method.startswith(\"regex\"):\n",
    "            m.regex_fallback_parses += 1\n",
    "        elif record.parse_method == \"default\":\n",
    "            m.default_fallback_parses += 1\n",
    "\n",
    "        if not record.response.strip():\n",
    "            m.empty_responses += 1\n",
    "        if fallback_used:\n",
    "            m.action_execution_failures += 1\n",
    "\n",
    "        action = executed_action.lower()\n",
    "        if action == \"fold\": m.fold_count += 1\n",
    "        elif action == \"check\": m.check_count += 1\n",
    "        elif action == \"call\": m.call_count += 1\n",
    "        elif action == \"raise\": m.raise_count += 1\n",
    "        elif action == \"all_in\": m.all_in_count += 1\n",
    "\n",
    "    def write_traces(self, matchup_id: str):\n",
    "        traces_dir = self.output_dir / \"observability\" / \"traces\"\n",
    "        traces_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for model_name, traces in self.traces.items():\n",
    "            safe_name = model_name.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "            filepath = traces_dir / f\"{safe_name}_{matchup_id}.jsonl\"\n",
    "            with open(filepath, \"w\") as f:\n",
    "                for trace in traces:\n",
    "                    f.write(json.dumps(trace) + \"\\n\")\n",
    "\n",
    "    def export_metrics(self):\n",
    "        metrics_path = self.output_dir / \"observability\" / \"model_metrics.json\"\n",
    "        data = {}\n",
    "        for name, m in self.metrics.items():\n",
    "            data[name] = {\n",
    "                \"total_actions\": m.total_actions,\n",
    "                \"valid_tag_parses\": m.valid_tag_parses,\n",
    "                \"regex_fallback_parses\": m.regex_fallback_parses,\n",
    "                \"default_fallback_parses\": m.default_fallback_parses,\n",
    "                \"parse_error_rate\": round(m.parse_error_rate, 4),\n",
    "                \"empty_responses\": m.empty_responses,\n",
    "                \"action_execution_failures\": m.action_execution_failures,\n",
    "                \"action_distribution\": {\n",
    "                    \"fold\": m.fold_count, \"check\": m.check_count,\n",
    "                    \"call\": m.call_count, \"raise\": m.raise_count, \"all_in\": m.all_in_count,\n",
    "                },\n",
    "                \"avg_latency_ms\": round(m.avg_latency_ms, 2),\n",
    "                \"p99_latency_ms\": round(m.p99_latency_ms, 2),\n",
    "                \"total_tokens\": m.total_tokens,\n",
    "            }\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def clear(self):\n",
    "        self.traces = {}\n",
    "        self.metrics = {}\n",
    "\n",
    "\n",
    "print(\"Observability loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============= TransformersPlayer =============\n\nclass TransformersPlayer:\n    SYSTEM_PROMPT = \"\"\"You are an expert poker player. Analyze the game state and decide your action.\n\nOutput format: <action>ACTION</action>\n- <action>f</action> = fold\n- <action>cc</action> = check or call\n- <action>cbr X</action> = bet or raise to X (multiple of big blind)\n\nVALID:\n<action>f</action>\n<action>cc</action>\n<action>cbr 6</action>\n\nINVALID:\n<action>fold</action> -- NOT PHH FORMAT\n<action>p6 cc</action> -- DO NOT SPECIFY PLAYER\n<action>cbr 1 5</action> -- INVALID PHH\n\nThink step by step, then output exactly ONE action tag.\"\"\"\n\n    THINK_END_TOKEN_ID = 151668\n\n    def __init__(self, name: str, model: Any, tokenizer: Any, temperature: float = 0.6, max_new_tokens: int = 512):\n        self.name = name\n        self.model = model\n        self.tokenizer = tokenizer\n        self.temperature = temperature\n        self.max_new_tokens = max_new_tokens\n        self.parser = ActionParser()\n        self.action_history: List[ActionRecord] = []\n        self._hand_id = 0\n        self._street = \"preflop\"\n\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def set_hand_context(self, hand_id: int, street: str):\n        self._hand_id = hand_id\n        self._street = street\n\n    def get_action(self, hole_cards, board, pot, to_call, stack, position, num_players) -> ParsedAction:\n        \"\"\"Get action using simple prompt (fallback).\"\"\"\n        start = time.perf_counter()\n        prompt = self._build_simple_prompt(hole_cards, board, pot, to_call, stack, position, num_players)\n\n        try:\n            thinking, response, tokens_gen = self._generate(prompt)\n            can_check = to_call == 0\n            result = self.parser.parse_with_metadata(response, can_check, stack)\n            action = result.action\n            parse_method = result.method\n            parse_error = result.error\n        except Exception as e:\n            thinking, response, tokens_gen = \"\", f\"ERROR: {e}\", 0\n            action = ParsedAction(\"fold\")\n            parse_method = \"error\"\n            parse_error = str(e)\n\n        latency = (time.perf_counter() - start) * 1000\n\n        self.action_history.append(ActionRecord(\n            hand_id=self._hand_id, street=self._street, hole_cards=hole_cards,\n            board=list(board), pot=pot, to_call=to_call, stack=stack,\n            position=position, action=action, thinking=thinking[:1000],\n            response=response[:500], latency_ms=latency, tokens_generated=tokens_gen,\n            parse_method=parse_method, parse_error=parse_error,\n        ))\n        return action\n\n    def get_action_with_prompt(self, prompt_text: str, hole_cards, board, pot, to_call, stack, position) -> ParsedAction:\n        \"\"\"Get action using a pre-built prompt (pokergpt format).\"\"\"\n        start = time.perf_counter()\n\n        # Format as chat with system prompt\n        messages = [\n            {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": prompt_text},\n        ]\n        full_prompt = self.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        try:\n            thinking, response, tokens_gen = self._generate(full_prompt)\n            can_check = to_call == 0\n            result = self.parser.parse_with_metadata(response, can_check, stack)\n            action = result.action\n            parse_method = result.method\n            parse_error = result.error\n        except Exception as e:\n            thinking, response, tokens_gen = \"\", f\"ERROR: {e}\", 0\n            action = ParsedAction(\"fold\")\n            parse_method = \"error\"\n            parse_error = str(e)\n\n        latency = (time.perf_counter() - start) * 1000\n\n        self.action_history.append(ActionRecord(\n            hand_id=self._hand_id, street=self._street, hole_cards=hole_cards,\n            board=list(board), pot=pot, to_call=to_call, stack=stack,\n            position=position, action=action, thinking=thinking[:1000],\n            response=response[:500], latency_ms=latency, tokens_generated=tokens_gen,\n            parse_method=parse_method, parse_error=parse_error,\n        ))\n        return action\n\n    def _build_simple_prompt(self, hole_cards, board, pot, to_call, stack, position, num_players) -> str:\n        \"\"\"Build simple prompt (fallback).\"\"\"\n        board_str = \" \".join(board) if board else \"None\"\n        user_msg = f\"\"\"Game: {num_players}-handed No-Limit Hold'em\nPosition: {position}\nStack: {stack}\nHole Cards: {hole_cards[0]} {hole_cards[1]}\nBoard: {board_str}\nPot: {pot}\nTo Call: {to_call}\n\nWhat is your action?\"\"\"\n        messages = [{\"role\": \"system\", \"content\": self.SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_msg}]\n        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    def _generate(self, prompt: str) -> Tuple[str, str, int]:\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n        input_len = inputs.input_ids.shape[1]\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs, max_new_tokens=self.max_new_tokens, temperature=self.temperature,\n                top_p=0.95, top_k=20, do_sample=True, pad_token_id=self.tokenizer.pad_token_id,\n            )\n\n        new_tokens = outputs[0][input_len:]\n        num_tokens = len(new_tokens)\n\n        try:\n            think_end_idx = (new_tokens == self.THINK_END_TOKEN_ID).nonzero(as_tuple=True)[0][-1].item()\n            thinking_tokens = new_tokens[:think_end_idx]\n            response_tokens = new_tokens[think_end_idx + 1:]\n        except:\n            thinking_tokens = torch.tensor([], dtype=new_tokens.dtype)\n            response_tokens = new_tokens\n\n        thinking = self.tokenizer.decode(thinking_tokens, skip_special_tokens=True).strip()\n        response = self.tokenizer.decode(response_tokens, skip_special_tokens=True).strip()\n        return thinking, response, num_tokens\n\n    def get_stats(self) -> dict:\n        if not self.action_history:\n            return {}\n        total = len(self.action_history)\n        preflop = [a for a in self.action_history if a.street == \"preflop\"]\n        vpip = len([a for a in preflop if a.action.action_type in (\"call\", \"raise\", \"all_in\")]) / len(preflop) if preflop else 0\n        pfr = len([a for a in preflop if a.action.action_type in (\"raise\", \"all_in\")]) / len(preflop) if preflop else 0\n        return {\n            \"total_actions\": total, \"vpip\": vpip, \"pfr\": pfr,\n            \"avg_latency_ms\": sum(a.latency_ms for a in self.action_history) / total,\n            \"fold_pct\": sum(1 for a in self.action_history if a.action.action_type == \"fold\") / total,\n        }\n\n    def get_last_record(self) -> Optional[ActionRecord]:\n        return self.action_history[-1] if self.action_history else None\n\n    def reset_history(self):\n        self.action_history = []\n\n\nprint(\"TransformersPlayer loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============= OpenAIPlayer =============\n\ntry:\n    from openai import OpenAI\n    OPENAI_AVAILABLE = True\nexcept ImportError:\n    OPENAI_AVAILABLE = False\n\n\nclass OpenAIPlayer:\n    SYSTEM_PROMPT = TransformersPlayer.SYSTEM_PROMPT  # Same prompt\n\n    def __init__(self, name: str, model: str = \"gpt-4\", temperature: float = 0.6, max_tokens: int = 512):\n        if not OPENAI_AVAILABLE:\n            raise ImportError(\"openai package not installed\")\n        self.name = name\n        self.model = model\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.client = OpenAI()\n        self.parser = ActionParser()\n        self.action_history: List[ActionRecord] = []\n        self._hand_id = 0\n        self._street = \"preflop\"\n        self.total_input_tokens = 0\n        self.total_output_tokens = 0\n\n    def set_hand_context(self, hand_id: int, street: str):\n        self._hand_id = hand_id\n        self._street = street\n\n    def get_action(self, hole_cards, board, pot, to_call, stack, position, num_players) -> ParsedAction:\n        \"\"\"Get action using simple prompt (fallback).\"\"\"\n        start = time.perf_counter()\n        user_msg = self._build_simple_prompt(hole_cards, board, pot, to_call, stack, position, num_players)\n\n        try:\n            response_text, tokens_in, tokens_out = self._call_api(user_msg)\n            can_check = to_call == 0\n            result = self.parser.parse_with_metadata(response_text, can_check, stack)\n            action = result.action\n            parse_method = result.method\n            parse_error = result.error\n        except Exception as e:\n            response_text = f\"ERROR: {e}\"\n            tokens_in = tokens_out = 0\n            action = ParsedAction(\"fold\")\n            parse_method = \"error\"\n            parse_error = str(e)\n\n        latency = (time.perf_counter() - start) * 1000\n\n        self.action_history.append(ActionRecord(\n            hand_id=self._hand_id, street=self._street, hole_cards=hole_cards,\n            board=list(board), pot=pot, to_call=to_call, stack=stack,\n            position=position, action=action, thinking=\"\",\n            response=response_text[:500], latency_ms=latency, tokens_generated=tokens_out,\n            parse_method=parse_method, parse_error=parse_error,\n        ))\n        return action\n\n    def get_action_with_prompt(self, prompt_text: str, hole_cards, board, pot, to_call, stack, position) -> ParsedAction:\n        \"\"\"Get action using a pre-built prompt (pokergpt format).\"\"\"\n        start = time.perf_counter()\n\n        try:\n            response_text, tokens_in, tokens_out = self._call_api(prompt_text)\n            can_check = to_call == 0\n            result = self.parser.parse_with_metadata(response_text, can_check, stack)\n            action = result.action\n            parse_method = result.method\n            parse_error = result.error\n        except Exception as e:\n            response_text = f\"ERROR: {e}\"\n            tokens_in = tokens_out = 0\n            action = ParsedAction(\"fold\")\n            parse_method = \"error\"\n            parse_error = str(e)\n\n        latency = (time.perf_counter() - start) * 1000\n\n        self.action_history.append(ActionRecord(\n            hand_id=self._hand_id, street=self._street, hole_cards=hole_cards,\n            board=list(board), pot=pot, to_call=to_call, stack=stack,\n            position=position, action=action, thinking=\"\",\n            response=response_text[:500], latency_ms=latency, tokens_generated=tokens_out,\n            parse_method=parse_method, parse_error=parse_error,\n        ))\n        return action\n\n    def _build_simple_prompt(self, hole_cards, board, pot, to_call, stack, position, num_players) -> str:\n        \"\"\"Build simple prompt (fallback).\"\"\"\n        board_str = \" \".join(board) if board else \"None\"\n        return f\"\"\"Game: {num_players}-handed No-Limit Hold'em\nPosition: {position}\nStack: {stack}\nHole Cards: {hole_cards[0]} {hole_cards[1]}\nBoard: {board_str}\nPot: {pot}\nTo Call: {to_call}\n\nWhat is your action?\"\"\"\n\n    def _call_api(self, user_msg: str) -> Tuple[str, int, int]:\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\"role\": \"system\", \"content\": self.SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user_msg},\n            ],\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n        )\n        content = response.choices[0].message.content or \"\"\n        tokens_in = response.usage.prompt_tokens if response.usage else 0\n        tokens_out = response.usage.completion_tokens if response.usage else 0\n        self.total_input_tokens += tokens_in\n        self.total_output_tokens += tokens_out\n        return content, tokens_in, tokens_out\n\n    def get_stats(self) -> dict:\n        if not self.action_history:\n            return {}\n        total = len(self.action_history)\n        preflop = [a for a in self.action_history if a.street == \"preflop\"]\n        vpip = len([a for a in preflop if a.action.action_type in (\"call\", \"raise\", \"all_in\")]) / len(preflop) if preflop else 0\n        pfr = len([a for a in preflop if a.action.action_type in (\"raise\", \"all_in\")]) / len(preflop) if preflop else 0\n        return {\n            \"total_actions\": total, \"vpip\": vpip, \"pfr\": pfr,\n            \"avg_latency_ms\": sum(a.latency_ms for a in self.action_history) / total,\n            \"fold_pct\": sum(1 for a in self.action_history if a.action.action_type == \"fold\") / total,\n            \"total_input_tokens\": self.total_input_tokens,\n            \"total_output_tokens\": self.total_output_tokens,\n        }\n\n    def get_last_record(self) -> Optional[ActionRecord]:\n        return self.action_history[-1] if self.action_history else None\n\n    def get_estimated_cost(self) -> float:\n        if \"turbo\" in self.model.lower() or \"4o\" in self.model.lower():\n            return self.total_input_tokens * 10 / 1e6 + self.total_output_tokens * 30 / 1e6\n        return self.total_input_tokens * 30 / 1e6 + self.total_output_tokens * 60 / 1e6\n\n    def reset_history(self):\n        self.action_history = []\n        self.total_input_tokens = 0\n        self.total_output_tokens = 0\n\n\nprint(f\"OpenAIPlayer loaded! (available: {OPENAI_AVAILABLE})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= Hand Result & Metrics =============\n",
    "\n",
    "@dataclass\n",
    "class HandResult:\n",
    "    hand_id: int\n",
    "    player_names: List[str]\n",
    "    starting_stacks: List[int]\n",
    "    ending_stacks: List[int]\n",
    "    chip_deltas: List[int]\n",
    "    hole_cards: Dict[str, Tuple[str, str]]\n",
    "    board: List[str]\n",
    "    winner_names: List[str]\n",
    "    pot_size: int\n",
    "\n",
    "\n",
    "class MetricsCollector:\n",
    "    def __init__(self, session_id: str = None):\n",
    "        self.session_id = session_id or f\"session_{int(time.time())}\"\n",
    "        self.hand_results: List[HandResult] = []\n",
    "        self.session_start = time.time()\n",
    "        self.player_summaries = {}\n",
    "\n",
    "    def log_hand(self, result: HandResult):\n",
    "        self.hand_results.append(result)\n",
    "\n",
    "    def finalize_session(self, player_stats: Dict[str, dict]):\n",
    "        duration = time.time() - self.session_start\n",
    "        total_hands = len(self.hand_results)\n",
    "\n",
    "        player_names = set()\n",
    "        for hr in self.hand_results:\n",
    "            player_names.update(hr.player_names)\n",
    "\n",
    "        for name in player_names:\n",
    "            hands_played = hands_won = total_chip_delta = 0\n",
    "            for hr in self.hand_results:\n",
    "                if name in hr.player_names:\n",
    "                    idx = hr.player_names.index(name)\n",
    "                    hands_played += 1\n",
    "                    total_chip_delta += hr.chip_deltas[idx]\n",
    "                    if name in hr.winner_names:\n",
    "                        hands_won += 1\n",
    "\n",
    "            self.player_summaries[name] = {\n",
    "                \"hands_played\": hands_played, \"hands_won\": hands_won,\n",
    "                \"win_rate\": hands_won / hands_played if hands_played > 0 else 0,\n",
    "                \"total_chip_delta\": total_chip_delta,\n",
    "                \"bb_per_100\": (total_chip_delta / hands_played * 100 / BIG_BLIND) if hands_played > 0 else 0,\n",
    "                **player_stats.get(name, {}),\n",
    "            }\n",
    "\n",
    "        self.duration = duration\n",
    "        self.total_hands = total_hands\n",
    "\n",
    "\n",
    "print(\"Metrics loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============= Eval Game with PromptBuilder =============\n\nclass EvalPokerGame:\n    def __init__(self, players, starting_stack=10000, small_blind=50, big_blind=100,\n                 metrics=None, observability=None, verbose=False, progress_callback=None,\n                 use_pokergpt_prompt=True):\n        self.players = players\n        self.num_players = len(players)\n        self.starting_stack = starting_stack\n        self.small_blind = small_blind\n        self.big_blind = big_blind\n        self.stacks = [starting_stack] * self.num_players\n        self.button = 0\n        self.hand_num = 0\n        self.metrics = metrics or MetricsCollector()\n        self.observability = observability\n        self.verbose = verbose\n        self.progress_callback = progress_callback\n        self.use_pokergpt_prompt = use_pokergpt_prompt\n\n        # PromptBuilder for pokergpt-style prompts\n        self.prompt_builder = PromptBuilder(big_blind=big_blind)\n\n    def play_session(self, num_hands: int) -> MetricsCollector:\n        for hand_idx in range(num_hands):\n            self._play_hand()\n            if self.progress_callback:\n                self.progress_callback(hand_idx + 1, num_hands)\n            if sum(1 for s in self.stacks if s > 0) < 2:\n                break\n        self.metrics.finalize_session({p.name: p.get_stats() for p in self.players})\n        return self.metrics\n\n    def _play_hand(self):\n        self.hand_num += 1\n        self.button = (self.button + 1) % self.num_players\n        for p in self.players:\n            p.set_hand_context(self.hand_num, \"preflop\")\n\n        sb_pos = (self.button + 1) % self.num_players\n        bb_pos = (self.button + 2) % self.num_players\n        if self.stacks[sb_pos] <= 0 or self.stacks[bb_pos] <= 0:\n            return\n\n        starting_stacks = self.stacks.copy()\n\n        # Reset prompt builder for new hand\n        self.prompt_builder.reset_hand()\n\n        try:\n            state = NoLimitTexasHoldem.create_state(\n                automations=(Automation.ANTE_POSTING, Automation.BET_COLLECTION, Automation.BLIND_OR_STRADDLE_POSTING,\n                             Automation.CARD_BURNING, Automation.HOLE_DEALING, Automation.HOLE_CARDS_SHOWING_OR_MUCKING,\n                             Automation.HAND_KILLING, Automation.CHIPS_PUSHING, Automation.CHIPS_PULLING),\n                ante_trimming_status=True, raw_antes={-1: 0},\n                raw_blinds_or_straddles=(self.small_blind, self.big_blind),\n                min_bet=self.big_blind, raw_starting_stacks=self.stacks.copy(), player_count=self.num_players,\n            )\n        except Exception as e:\n            if self.verbose:\n                print(f\"Error: {e}\")\n            return\n\n        hole_cards = [(str(state.hole_cards[i][0]), str(state.hole_cards[i][1]))\n                      if state.hole_cards[i] and len(state.hole_cards[i]) >= 2 else (\"??\", \"??\")\n                      for i in range(self.num_players)]\n        dealable = list(state.get_dealable_cards())\n        random.shuffle(dealable)\n        deck = dealable\n        board = []\n\n        # Get positions for this hand\n        positions = [get_position_name(i, self.num_players, self.button) for i in range(self.num_players)]\n\n        # Record initial deals in prompt builder\n        for i in range(self.num_players):\n            is_sb = i == sb_pos\n            is_bb = i == bb_pos\n            blind_note = \"\"\n            if is_sb:\n                blind_note = f\"Small Blind {self.small_blind / self.big_blind:.1f} BB\"\n            elif is_bb:\n                blind_note = f\"Big Blind {self.big_blind / self.big_blind:.1f} BB\"\n            # For now, record deals without hero designation (hero changes per action)\n            self.prompt_builder.action_history.append(\n                f\"{positions[i]} was dealt hole cards\" + (f\" ({blind_note})\" if blind_note else \"\") + \".\"\n            )\n\n        for street in [\"preflop\", \"flop\", \"turn\", \"river\"]:\n            if state.status is False:\n                break\n            for p in self.players:\n                p.set_hand_context(self.hand_num, street)\n\n            if street == \"flop\":\n                board = [deck.pop(), deck.pop(), deck.pop()]\n                for c in board:\n                    try: state.deal_board(c)\n                    except: pass\n                self.prompt_builder.record_board([str(c) for c in board])\n            elif street in (\"turn\", \"river\"):\n                board.append(deck.pop())\n                try: state.deal_board(board[-1])\n                except: pass\n                self.prompt_builder.record_board([str(c) for c in board])\n\n            board_strs = [str(c) for c in board]\n            while state.actor_index is not None:\n                actor = state.actor_index\n                player = self.players[actor]\n                pot = state.total_pot_amount if hasattr(state, 'total_pot_amount') else 0\n                current_bet = max(state.bets) if state.bets else 0\n                player_bet = state.bets[actor] if state.bets else 0\n                to_call = current_bet - player_bet\n                stack = state.stacks[actor]\n                position = positions[actor]\n\n                # Build pokergpt-style prompt\n                if self.use_pokergpt_prompt and hasattr(player, 'get_action_with_prompt'):\n                    # Get min raise (current bet + big blind as simplified rule)\n                    min_raise = current_bet + self.big_blind if current_bet > 0 else self.big_blind\n\n                    prompt_text = self.prompt_builder.build_prompt(\n                        hero_idx=actor,\n                        hero_cards=hole_cards[actor],\n                        board=board_strs,\n                        stacks=list(state.stacks),\n                        bets=list(state.bets) if state.bets else [0] * self.num_players,\n                        pot=pot,\n                        to_call=to_call,\n                        min_raise=min_raise,\n                        button_idx=self.button,\n                        num_players=self.num_players,\n                        street=street,\n                    )\n                    action = player.get_action_with_prompt(\n                        prompt_text, hole_cards[actor], board_strs, pot, to_call, stack, position\n                    )\n                else:\n                    action = player.get_action(hole_cards[actor], board_strs, pot, to_call, stack, position, self.num_players)\n\n                if self.verbose:\n                    print(f\"  H{self.hand_num} {street} {player.name}: {action}\")\n\n                executed, fallback = self._execute_action(state, action)\n\n                # Record action in prompt builder for subsequent players\n                player_label = positions[actor]\n                if executed == \"fold\":\n                    self.prompt_builder.record_action(player_label, \"folded\")\n                elif executed == \"check\":\n                    self.prompt_builder.record_action(player_label, \"checked\")\n                elif executed == \"call\":\n                    self.prompt_builder.record_action(player_label, \"called\", to_call / self.big_blind)\n                elif executed == \"raise\":\n                    raise_amount = action.amount if action.amount else stack\n                    self.prompt_builder.record_action(player_label, \"bet/raised to\", raise_amount / self.big_blind)\n                elif executed == \"all_in\":\n                    self.prompt_builder.record_action(player_label, \"went all-in\", stack / self.big_blind)\n\n                # Record observability\n                if self.observability:\n                    record = player.get_last_record()\n                    if record:\n                        self.observability.record_action(player.name, record, executed, fallback)\n\n        if hasattr(state, 'stacks'):\n            for i in range(self.num_players):\n                self.stacks[i] = state.stacks[i]\n\n        chip_deltas = [self.stacks[i] - starting_stacks[i] for i in range(self.num_players)]\n        winners = [self.players[i].name for i, d in enumerate(chip_deltas) if d > 0]\n\n        self.metrics.log_hand(HandResult(\n            hand_id=self.hand_num, player_names=[p.name for p in self.players],\n            starting_stacks=starting_stacks, ending_stacks=self.stacks.copy(),\n            chip_deltas=chip_deltas, hole_cards={p.name: hole_cards[i] for i, p in enumerate(self.players)},\n            board=[str(c) for c in board], winner_names=winners, pot_size=sum(abs(d) for d in chip_deltas if d < 0),\n        ))\n\n    def _execute_action(self, state, action: ParsedAction) -> Tuple[str, bool]:\n        \"\"\"Execute action, return (executed_action_name, used_fallback)\"\"\"\n        try:\n            if action.action_type == \"fold\":\n                state.fold()\n                return \"fold\", False\n            elif action.action_type in (\"check\", \"call\"):\n                state.check_or_call()\n                return action.action_type, False\n            elif action.action_type in (\"raise\", \"bet\"):\n                state.complete_bet_or_raise_to(action.amount)\n                return \"raise\", False\n            elif action.action_type == \"all_in\":\n                actor = state.actor_index\n                state.complete_bet_or_raise_to(state.stacks[actor] + state.bets[actor])\n                return \"all_in\", False\n        except:\n            pass\n\n        # Fallback\n        try:\n            state.check_or_call()\n            return \"call\", True\n        except:\n            try:\n                state.fold()\n                return \"fold\", True\n            except:\n                return \"error\", True\n\n\nprint(\"EvalPokerGame loaded with pokergpt prompt support!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_transformers_model(name: str, model_id: str):\n",
    "    \"\"\"Load a HuggingFace model at full weight (FP16).\"\"\"\n",
    "    print(f\"Loading {name}: {model_id} (FP16 - full weight)...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"  Loaded. VRAM: {allocated:.1f}GB\")\n",
    "\n",
    "    return TransformersPlayer(name, model, tokenizer)\n",
    "\n",
    "\n",
    "def unload_model(player):\n",
    "    \"\"\"Unload model to free VRAM.\"\"\"\n",
    "    if hasattr(player, 'model'):\n",
    "        del player.model\n",
    "        del player.tokenizer\n",
    "    del player\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"  Model unloaded. VRAM: {torch.cuda.memory_allocated() / 1024**3:.1f}GB\")\n",
    "\n",
    "\n",
    "print(\"Model loading functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "# Tournament state\n",
    "matchup_results = []\n",
    "champion = None\n",
    "observability = ObservabilityCollector(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "def run_matchup(p1_name: str, p2_name: str, round_name: str) -> Tuple[str, dict]:\n",
    "    \"\"\"Run a single matchup. Returns (winner_name, result_dict).\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{round_name}: {p1_name} vs {p2_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load players\n",
    "    if MODELS[p1_name][\"type\"] == \"openai\":\n",
    "        p1 = OpenAIPlayer(p1_name, model=MODELS[p1_name].get(\"model\", \"gpt-4\"))\n",
    "    else:\n",
    "        p1 = load_transformers_model(p1_name, MODELS[p1_name][\"model_id\"])\n",
    "\n",
    "    if MODELS[p2_name][\"type\"] == \"openai\":\n",
    "        p2 = OpenAIPlayer(p2_name, model=MODELS[p2_name].get(\"model\", \"gpt-4\"))\n",
    "    else:\n",
    "        p2 = load_transformers_model(p2_name, MODELS[p2_name][\"model_id\"])\n",
    "\n",
    "    # Create game\n",
    "    metrics = MetricsCollector(f\"{round_name}_{p1_name}_vs_{p2_name}\")\n",
    "    pbar = tqdm(total=HANDS_PER_MATCHUP, desc=f\"{p1_name} vs {p2_name}\")\n",
    "\n",
    "    def update_progress(current, total):\n",
    "        pbar.n = current\n",
    "        pbar.refresh()\n",
    "\n",
    "    game = EvalPokerGame(\n",
    "        players=[p1, p2],\n",
    "        starting_stack=STARTING_STACK,\n",
    "        small_blind=SMALL_BLIND,\n",
    "        big_blind=BIG_BLIND,\n",
    "        metrics=metrics,\n",
    "        observability=observability,\n",
    "        verbose=VERBOSE,\n",
    "        progress_callback=update_progress,\n",
    "    )\n",
    "\n",
    "    # Run matchup\n",
    "    result = game.play_session(HANDS_PER_MATCHUP)\n",
    "    pbar.close()\n",
    "\n",
    "    # Write traces\n",
    "    observability.write_traces(f\"{round_name.replace(' ', '_')}\")\n",
    "\n",
    "    # Determine winner\n",
    "    p1_delta = result.player_summaries[p1_name][\"total_chip_delta\"]\n",
    "    p2_delta = result.player_summaries[p2_name][\"total_chip_delta\"]\n",
    "\n",
    "    if p1_delta > p2_delta:\n",
    "        winner = p1_name\n",
    "    elif p2_delta > p1_delta:\n",
    "        winner = p2_name\n",
    "    else:\n",
    "        # Tiebreaker: hands won\n",
    "        p1_wins = result.player_summaries[p1_name][\"hands_won\"]\n",
    "        p2_wins = result.player_summaries[p2_name][\"hands_won\"]\n",
    "        winner = p1_name if p1_wins >= p2_wins else p2_name\n",
    "\n",
    "    # Print result\n",
    "    print(f\"\\n{round_name} Result:\")\n",
    "    print(f\"  {p1_name}: {p1_delta:+} chips (BB/100: {result.player_summaries[p1_name]['bb_per_100']:+.2f})\")\n",
    "    print(f\"  {p2_name}: {p2_delta:+} chips (BB/100: {result.player_summaries[p2_name]['bb_per_100']:+.2f})\")\n",
    "    print(f\"  WINNER: {winner}\")\n",
    "\n",
    "    result_dict = {\n",
    "        \"round\": round_name,\n",
    "        \"player1\": p1_name,\n",
    "        \"player2\": p2_name,\n",
    "        \"player1_chips\": p1_delta,\n",
    "        \"player2_chips\": p2_delta,\n",
    "        \"player1_bb100\": result.player_summaries[p1_name][\"bb_per_100\"],\n",
    "        \"player2_bb100\": result.player_summaries[p2_name][\"bb_per_100\"],\n",
    "        \"winner\": winner,\n",
    "        \"hands_played\": result.total_hands,\n",
    "    }\n",
    "\n",
    "    # Unload models to free VRAM\n",
    "    if MODELS[p1_name][\"type\"] == \"transformers\":\n",
    "        unload_model(p1)\n",
    "    if MODELS[p2_name][\"type\"] == \"transformers\":\n",
    "        unload_model(p2)\n",
    "\n",
    "    return winner, result_dict\n",
    "\n",
    "\n",
    "print(\"Tournament ready to start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the gauntlet tournament\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POKER LLM TOURNAMENT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Format: Gauntlet (1000 hands per matchup)\")\n",
    "print(f\"Winner: Total chip profit\\n\")\n",
    "\n",
    "# Round 1: Qwen3-Base vs Qwen3-SFT\n",
    "r1_winner, r1_result = run_matchup(\"Qwen3-Base\", \"Qwen3-SFT\", \"Round 1\")\n",
    "matchup_results.append(r1_result)\n",
    "\n",
    "# Round 2: Winner R1 vs Llama3-SFT\n",
    "r2_winner, r2_result = run_matchup(r1_winner, \"Llama3-SFT\", \"Round 2\")\n",
    "matchup_results.append(r2_result)\n",
    "\n",
    "# Round 3: Only if your model beat Llama3\n",
    "if r2_winner != \"Llama3-SFT\" and os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(f\"\\n{r2_winner} beat Llama3-SFT! Proceeding to GPT-4 matchup...\")\n",
    "    r3_winner, r3_result = run_matchup(r2_winner, \"GPT-4\", \"Round 3\")\n",
    "    matchup_results.append(r3_result)\n",
    "    champion = r3_winner\n",
    "elif r2_winner == \"Llama3-SFT\":\n",
    "    print(f\"\\nLlama3-SFT won Round 2. Skipping GPT-4 matchup (cost savings).\")\n",
    "    champion = \"Llama3-SFT\"\n",
    "else:\n",
    "    print(f\"\\nNo OpenAI API key. Skipping GPT-4 matchup.\")\n",
    "    champion = r2_winner\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"TOURNAMENT CHAMPION: {champion}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Observability Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Export metrics\n",
    "observability.export_metrics()\n",
    "\n",
    "# Error Rate Summary Table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBSERVABILITY: ERROR RATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "error_rows = []\n",
    "for name, m in observability.metrics.items():\n",
    "    error_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Actions\": m.total_actions,\n",
    "        \"Valid Parse\": m.valid_tag_parses,\n",
    "        \"Regex Fallback\": m.regex_fallback_parses,\n",
    "        \"Default Fallback\": m.default_fallback_parses,\n",
    "        \"Error Rate\": f\"{m.parse_error_rate:.1%}\",\n",
    "        \"Exec Failures\": m.action_execution_failures,\n",
    "    })\n",
    "\n",
    "df_errors = pd.DataFrame(error_rows)\n",
    "print(df_errors.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "df_errors.to_csv(f\"{OUTPUT_DIR}/observability/error_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Action Distribution Chart\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# 1. Parse Error Rate\n",
    "ax = axes[0]\n",
    "models = list(observability.metrics.keys())\n",
    "error_rates = [m.parse_error_rate * 100 for m in observability.metrics.values()]\n",
    "colors = [\"green\" if r < 5 else \"orange\" if r < 15 else \"red\" for r in error_rates]\n",
    "ax.bar(models, error_rates, color=colors)\n",
    "ax.set_title(\"Parse Error Rate\", fontsize=14)\n",
    "ax.set_ylabel(\"Error Rate (%)\")\n",
    "ax.set_ylim(0, max(error_rates) * 1.2 if error_rates else 10)\n",
    "\n",
    "# 2. Action Distribution\n",
    "ax = axes[1]\n",
    "action_data = {}\n",
    "for name, m in observability.metrics.items():\n",
    "    total = m.total_actions or 1\n",
    "    action_data[name] = {\n",
    "        \"Fold\": m.fold_count / total * 100,\n",
    "        \"Check\": m.check_count / total * 100,\n",
    "        \"Call\": m.call_count / total * 100,\n",
    "        \"Raise\": m.raise_count / total * 100,\n",
    "        \"All-in\": m.all_in_count / total * 100,\n",
    "    }\n",
    "\n",
    "df_actions = pd.DataFrame(action_data).T\n",
    "df_actions.plot(kind=\"bar\", stacked=True, ax=ax, colormap=\"Set3\")\n",
    "ax.set_title(\"Action Distribution\", fontsize=14)\n",
    "ax.set_ylabel(\"%\")\n",
    "ax.legend(loc=\"upper right\", fontsize=8)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "# 3. Latency\n",
    "ax = axes[2]\n",
    "latency_data = {name: m.latencies for name, m in observability.metrics.items()}\n",
    "ax.boxplot(latency_data.values(), labels=latency_data.keys())\n",
    "ax.set_title(\"Latency Distribution\", fontsize=14)\n",
    "ax.set_ylabel(\"Latency (ms)\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/charts/observability.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tournament Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matchup Results Table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOURNAMENT RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_matchups = pd.DataFrame(matchup_results)\n",
    "print(df_matchups[[\"round\", \"player1\", \"player2\", \"player1_chips\", \"player2_chips\", \"winner\"]].to_string(index=False))\n",
    "\n",
    "# Save\n",
    "df_matchups.to_csv(f\"{OUTPUT_DIR}/matchups.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Chip progression per matchup\n",
    "ax = axes[0]\n",
    "for i, r in enumerate(matchup_results):\n",
    "    x = [i, i]\n",
    "    y = [r[\"player1_chips\"], r[\"player2_chips\"]]\n",
    "    colors = [\"green\" if c > 0 else \"red\" for c in y]\n",
    "    ax.bar([f\"{r['player1']}\\n({r['round']})\", f\"{r['player2']}\\n({r['round']})\"],\n",
    "           [r[\"player1_chips\"], r[\"player2_chips\"]], color=colors, alpha=0.7)\n",
    "\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_title(\"Chip Results by Matchup\", fontsize=14)\n",
    "ax.set_ylabel(\"Chip Delta\")\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=9)\n",
    "\n",
    "# BB/100 comparison\n",
    "ax = axes[1]\n",
    "all_players = set()\n",
    "player_bb100 = {}\n",
    "for r in matchup_results:\n",
    "    if r[\"player1\"] not in player_bb100:\n",
    "        player_bb100[r[\"player1\"]] = []\n",
    "    if r[\"player2\"] not in player_bb100:\n",
    "        player_bb100[r[\"player2\"]] = []\n",
    "    player_bb100[r[\"player1\"]].append(r[\"player1_bb100\"])\n",
    "    player_bb100[r[\"player2\"]].append(r[\"player2_bb100\"])\n",
    "\n",
    "avg_bb100 = {p: sum(v)/len(v) for p, v in player_bb100.items()}\n",
    "colors = [\"green\" if v > 0 else \"red\" for v in avg_bb100.values()]\n",
    "ax.bar(avg_bb100.keys(), avg_bb100.values(), color=colors)\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_title(\"Average BB/100 by Model\", fontsize=14)\n",
    "ax.set_ylabel(\"BB/100\")\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/charts/tournament_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Blog-Ready Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BLOG_SUMMARY.md\n",
    "blog_md = f\"\"\"# Poker LLM Tournament Results\n",
    "\n",
    "## Champion: {champion}\n",
    "\n",
    "### Tournament Bracket\n",
    "| Round | Matchup | Winner | Chip Differential |\n",
    "|-------|---------|--------|-------------------|\n",
    "\"\"\"\n",
    "\n",
    "for r in matchup_results:\n",
    "    winner_chips = r[\"player1_chips\"] if r[\"winner\"] == r[\"player1\"] else r[\"player2_chips\"]\n",
    "    blog_md += f\"| {r['round']} | {r['player1']} vs {r['player2']} | {r['winner']} | {winner_chips:+} |\\n\"\n",
    "\n",
    "blog_md += f\"\"\"\n",
    "### Key Statistics\n",
    "- **Total hands played**: {sum(r['hands_played'] for r in matchup_results)}\n",
    "- **Matchups completed**: {len(matchup_results)}\n",
    "\"\"\"\n",
    "\n",
    "# Add model comparison\n",
    "blog_md += \"\\n### Model Performance\\n\"\n",
    "blog_md += \"| Model | Avg BB/100 | Error Rate |\\n\"\n",
    "blog_md += \"|-------|------------|------------|\\n\"\n",
    "for name, bb in avg_bb100.items():\n",
    "    err = observability.metrics.get(name)\n",
    "    err_rate = f\"{err.parse_error_rate:.1%}\" if err else \"N/A\"\n",
    "    blog_md += f\"| {name} | {bb:+.2f} | {err_rate} |\\n\"\n",
    "\n",
    "blog_md += f\"\"\"\n",
    "### Notable Findings\n",
    "- Champion **{champion}** emerged victorious after {len(matchup_results)} rounds\n",
    "\"\"\"\n",
    "\n",
    "if \"Llama3-SFT\" in [r[\"winner\"] for r in matchup_results if r[\"round\"] == \"Round 2\"]:\n",
    "    blog_md += \"- The PokerBench paper model (Llama3-SFT) proved superior to custom fine-tunes\\n\"\n",
    "else:\n",
    "    blog_md += f\"- Custom fine-tuned model beat the PokerBench benchmark (Llama3-SFT)\\n\"\n",
    "\n",
    "# Write file\n",
    "with open(f\"{OUTPUT_DIR}/BLOG_SUMMARY.md\", \"w\") as f:\n",
    "    f.write(blog_md)\n",
    "\n",
    "print(blog_md)\n",
    "print(f\"\\nSaved to: {OUTPUT_DIR}/BLOG_SUMMARY.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quotable Stats (for easy copy-paste)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUOTABLE STATS (copy-paste ready)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Champion: {champion}\")\n",
    "\n",
    "if len(matchup_results) > 0:\n",
    "    final = matchup_results[-1]\n",
    "    margin = abs(final[\"player1_chips\"] - final[\"player2_chips\"])\n",
    "    print(f\"Final margin: {margin:,} chips ({margin // BIG_BLIND} BB)\")\n",
    "\n",
    "# Best local model\n",
    "local_models = [\"Qwen3-Base\", \"Qwen3-SFT\"]\n",
    "local_bb = {m: avg_bb100.get(m, 0) for m in local_models if m in avg_bb100}\n",
    "if local_bb:\n",
    "    best_local = max(local_bb, key=local_bb.get)\n",
    "    print(f\"Best local model: {best_local} (BB/100: {local_bb[best_local]:+.2f})\")\n",
    "\n",
    "# Error rates\n",
    "print(\"\\nError rates:\")\n",
    "for name, m in sorted(observability.metrics.items(), key=lambda x: x[1].parse_error_rate):\n",
    "    print(f\"  {name}: {m.parse_error_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Save tournament.json\n",
    "tournament_data = {\n",
    "    \"champion\": champion,\n",
    "    \"config\": {\n",
    "        \"hands_per_matchup\": HANDS_PER_MATCHUP,\n",
    "        \"starting_stack\": STARTING_STACK,\n",
    "        \"blinds\": f\"{SMALL_BLIND}/{BIG_BLIND}\",\n",
    "        \"gpu\": GPU_NAME,\n",
    "    },\n",
    "    \"matchups\": matchup_results,\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/tournament.json\", \"w\") as f:\n",
    "    json.dump(tournament_data, f, indent=2)\n",
    "\n",
    "# Copy to Drive\n",
    "drive_path = \"/content/drive/MyDrive/poker_tournament_results\"\n",
    "shutil.copytree(OUTPUT_DIR, drive_path, dirs_exist_ok=True)\n",
    "\n",
    "print(f\"Results exported to Google Drive: {drive_path}/\")\n",
    "print(f\"\\nFiles:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    print(f\"  - {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}