{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poker Bot Evaluation\n",
    "\n",
    "Evaluate `YiPz/Qwen3-4B-pokerbench-sft` vs base `unsloth/Qwen3-4B-Thinking-2507`\n",
    "\n",
    "**Quick Start:** Run all cells in order. Results saved to `/content/eval_results/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for caching\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes torch pokerkit tqdm pandas matplotlib\n",
    "\n",
    "# Clone and install poker bot package\n",
    "!git clone https://github.com/yilenpan/player_poker_bot.git /content/player_poker_bot 2>/dev/null || true\n",
    "!pip install -q -e /content/player_poker_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from src.eval import (\n",
    "    HardwareConfig,\n",
    "    EvalConfig,\n",
    "    ModelConfig,\n",
    "    TransformersPlayer,\n",
    "    MetricsCollector,\n",
    "    EvalPokerGame,\n",
    ")\n",
    "\n",
    "print(\"Imports loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect hardware\n",
    "hw = HardwareConfig.detect()\n",
    "print(f\"GPU: {hw.gpu_name} ({hw.vram_gb:.0f}GB)\")\n",
    "print(f\"Quantization: {hw.quantization.value}\")\n",
    "\n",
    "# Experiment config\n",
    "NUM_HANDS = 100\n",
    "NUM_SESSIONS = 3\n",
    "STARTING_STACK = 10000\n",
    "SMALL_BLIND = 50\n",
    "BIG_BLIND = 100\n",
    "SEED = 42\n",
    "\n",
    "MODELS = {\n",
    "    \"SFT\": \"YiPz/Qwen3-4B-pokerbench-sft\",\n",
    "    \"Base\": \"unsloth/Qwen3-4B-Thinking-2507\",\n",
    "}\n",
    "\n",
    "os.makedirs(\"/content/eval_results\", exist_ok=True)\n",
    "print(f\"\\nConfig: {NUM_HANDS} hands Ã— {NUM_SESSIONS} sessions = {NUM_HANDS * NUM_SESSIONS} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models = {}\n",
    "tokenizers = {}\n",
    "bnb_config = hw.get_bnb_config()\n",
    "\n",
    "for name, model_id in MODELS.items():\n",
    "    print(f\"Loading {name}: {model_id}...\")\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    \n",
    "    load_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True, \"torch_dtype\": torch.float16}\n",
    "    if bnb_config:\n",
    "        load_kwargs[\"quantization_config\"] = bnb_config\n",
    "    \n",
    "    loaded_models[name] = AutoModelForCausalLM.from_pretrained(model_id, **load_kwargs)\n",
    "    print(f\"  VRAM: {torch.cuda.memory_allocated() / 1024**3:.1f}GB\")\n",
    "\n",
    "print(f\"\\nTotal VRAM: {torch.cuda.memory_allocated() / 1024**3:.1f}GB / {hw.vram_gb:.0f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "all_results = []\n",
    "\n",
    "for session_idx in range(NUM_SESSIONS):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Session {session_idx + 1}/{NUM_SESSIONS}\")\n",
    "    \n",
    "    players = [\n",
    "        TransformersPlayer(\"SFT\", loaded_models[\"SFT\"], tokenizers[\"SFT\"]),\n",
    "        TransformersPlayer(\"Base\", loaded_models[\"Base\"], tokenizers[\"Base\"]),\n",
    "    ]\n",
    "    \n",
    "    metrics = MetricsCollector(f\"session_{session_idx}\")\n",
    "    pbar = tqdm(total=NUM_HANDS, desc=f\"Session {session_idx+1}\")\n",
    "    \n",
    "    game = EvalPokerGame(\n",
    "        players=players,\n",
    "        starting_stack=STARTING_STACK,\n",
    "        small_blind=SMALL_BLIND,\n",
    "        big_blind=BIG_BLIND,\n",
    "        metrics=metrics,\n",
    "        progress_callback=lambda cur, tot: (setattr(pbar, 'n', cur), pbar.refresh()),\n",
    "    )\n",
    "    \n",
    "    result = game.play_session(NUM_HANDS)\n",
    "    pbar.close()\n",
    "    all_results.append(result)\n",
    "    \n",
    "    for name, stats in result.player_summaries.items():\n",
    "        print(f\"  {name}: {stats['hands_won']}/{stats['hands_played']} wins, BB/100: {stats['bb_per_100']:+.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nALL SESSIONS COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "aggregate = {}\n",
    "for result in all_results:\n",
    "    for name, stats in result.player_summaries.items():\n",
    "        if name not in aggregate:\n",
    "            aggregate[name] = {\"hands_played\": 0, \"hands_won\": 0, \"total_chip_delta\": 0, \n",
    "                               \"vpip_sum\": 0, \"pfr_sum\": 0, \"sessions\": 0}\n",
    "        aggregate[name][\"hands_played\"] += stats[\"hands_played\"]\n",
    "        aggregate[name][\"hands_won\"] += stats[\"hands_won\"]\n",
    "        aggregate[name][\"total_chip_delta\"] += stats[\"total_chip_delta\"]\n",
    "        aggregate[name][\"vpip_sum\"] += stats.get(\"vpip\", 0)\n",
    "        aggregate[name][\"pfr_sum\"] += stats.get(\"pfr\", 0)\n",
    "        aggregate[name][\"sessions\"] += 1\n",
    "\n",
    "rows = []\n",
    "for name, agg in aggregate.items():\n",
    "    hp = agg[\"hands_played\"]\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Hands\": hp,\n",
    "        \"Win%\": agg[\"hands_won\"] / hp * 100 if hp else 0,\n",
    "        \"BB/100\": agg[\"total_chip_delta\"] / hp * 100 / BIG_BLIND if hp else 0,\n",
    "        \"VPIP%\": agg[\"vpip_sum\"] / agg[\"sessions\"] * 100 if agg[\"sessions\"] else 0,\n",
    "        \"PFR%\": agg[\"pfr_sum\"] / agg[\"sessions\"] * 100 if agg[\"sessions\"] else 0,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values(\"BB/100\", ascending=False)\n",
    "print(df.to_string(index=False, float_format=\"%.2f\"))\n",
    "\n",
    "# SFT improvement\n",
    "if \"SFT\" in aggregate and \"Base\" in aggregate:\n",
    "    sft_bb = aggregate[\"SFT\"][\"total_chip_delta\"] / aggregate[\"SFT\"][\"hands_played\"] * 100 / BIG_BLIND\n",
    "    base_bb = aggregate[\"Base\"][\"total_chip_delta\"] / aggregate[\"Base\"][\"hands_played\"] * 100 / BIG_BLIND\n",
    "    print(f\"\\nSFT improvement: {sft_bb - base_bb:+.2f} BB/100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# BB/100\n",
    "colors = [\"green\" if x >= 0 else \"red\" for x in df[\"BB/100\"]]\n",
    "axes[0].bar(df[\"Model\"], df[\"BB/100\"], color=colors)\n",
    "axes[0].axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "axes[0].set_title(\"Profitability (BB/100)\")\n",
    "\n",
    "# Win rate\n",
    "axes[1].bar(df[\"Model\"], df[\"Win%\"], color=\"steelblue\")\n",
    "axes[1].axhline(y=50, color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
    "axes[1].set_title(\"Win Rate (%)\")\n",
    "\n",
    "# Playing style\n",
    "x = range(len(df))\n",
    "axes[2].bar([i - 0.175 for i in x], df[\"VPIP%\"], 0.35, label=\"VPIP\", color=\"orange\")\n",
    "axes[2].bar([i + 0.175 for i in x], df[\"PFR%\"], 0.35, label=\"PFR\", color=\"purple\")\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(df[\"Model\"])\n",
    "axes[2].set_title(\"Playing Style\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/content/eval_results/comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/content/eval_results/summary.csv\", index=False)\n",
    "\n",
    "results_json = {\n",
    "    \"config\": {\n",
    "        \"num_hands\": NUM_HANDS, \"num_sessions\": NUM_SESSIONS,\n",
    "        \"starting_stack\": STARTING_STACK, \"blinds\": f\"{SMALL_BLIND}/{BIG_BLIND}\",\n",
    "        \"gpu\": hw.gpu_name, \"quantization\": hw.quantization.value,\n",
    "    },\n",
    "    \"aggregate\": df.to_dict(orient=\"records\"),\n",
    "    \"sessions\": [{\"session_id\": r.session_id, \"hands\": r.total_hands} for r in all_results],\n",
    "}\n",
    "\n",
    "with open(\"/content/eval_results/results.json\", \"w\") as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "\n",
    "print(\"Saved to /content/eval_results/: summary.csv, results.json, comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
